# Python Advanced Techniques 

## 1. æ‰§è¡Œæ¨¡å‹

### 1.1 è¿è¡Œæœºåˆ¶

#### 1.1.1 Pythonè§£é‡Šå™¨å¦‚ä½•å·¥ä½œ
Python è§£é‡Šå™¨ä¸æ­¢ä¸€ç§ï¼Œæœ‰ **CPythonã€IPythonã€Jythonã€PyPy**ç­‰ã€‚é¡¾åæ€ä¹‰ï¼ŒCPython å°±æ˜¯ç”¨ C è¯­è¨€å¼€å‘çš„äº†ï¼Œæ˜¯å®˜æ–¹æ ‡å‡†å®ç°ï¼Œæ‹¥æœ‰è‰¯å¥½çš„ç”Ÿæ€ï¼Œæ‰€ä»¥åº”ç”¨ä¹Ÿå°±æœ€ä¸ºå¹¿æ³›äº†ã€‚è€Œ IPython æ˜¯åœ¨ CPython çš„åŸºç¡€ä¹‹ä¸Šåœ¨äº¤äº’å¼æ–¹é¢å¾—åˆ°å¢å¼ºçš„è§£é‡Šå™¨ã€‚Jython æ˜¯ä¸“ä¸º Java å¹³å°è®¾è®¡çš„ Python è§£é‡Šå™¨ï¼Œå®ƒæŠŠ Python ä»£ç ç¼–è¯‘æˆ Java å­—èŠ‚ç æ‰§è¡Œã€‚PyPy æ˜¯ Python è¯­è¨€ï¼ˆ2.7.13å’Œ3.5.3ï¼‰çš„ä¸€ç§å¿«é€Ÿã€å…¼å®¹çš„æ›¿ä»£å®ç°ï¼Œä»¥é€Ÿåº¦å¿«è‘—ç§°ã€‚


**å­—èŠ‚ç **æ˜¯ä»‹äºæºä»£ç å’Œæœºå™¨ç ä¹‹é—´çš„ä¸­é—´ä»£ç ï¼Œç”±ç¼–è¯‘å™¨å°†é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼ˆå¦‚ Javaã€Pythonï¼‰çš„æºä»£ç ç¼–è¯‘ç”Ÿæˆï¼Œä»¥å­—èŠ‚ï¼ˆ8 ä½ï¼‰ä¸ºåŸºæœ¬å•ä½çš„æŒ‡ä»¤é›†ã€‚å®ƒä¸ä¾èµ–å…·ä½“ç¡¬ä»¶æ¶æ„ï¼Œéœ€é€šè¿‡è™šæ‹Ÿæœºï¼ˆVMï¼‰è§£é‡Šæˆ–ç¼–è¯‘ä¸ºæœºå™¨ç åæ‰§è¡Œã€‚


#### 1.1.2 CPython è§£é‡Šå™¨å¦‚ä½•æ‰§è¡Œä»£ç 
Python ä»£ç åœ¨æ‰§è¡Œå‰ä¼šç»è¿‡ä»¥ä¸‹æ­¥éª¤ï¼š

1. **æºä»£ç ** â†’ **å­—èŠ‚ç **ï¼ˆç¼–è¯‘ï¼‰
2. **å­—èŠ‚ç ** â†’ **æœºå™¨ç **ï¼ˆè§£é‡Šæ‰§è¡Œï¼‰

```python
import dis

def hello():
    x = 5
    y = 10
    return x + y

# åç¼–è¯‘æŸ¥çœ‹å­—èŠ‚ç 
dis.dis(hello)
```
ç»“æœå¦‚ä¸‹ï¼š
```
  3           0 RESUME                   0

  4           2 LOAD_CONST               1 (5)
              4 STORE_FAST               0 (x)

  5           6 LOAD_CONST               2 (10)
              8 STORE_FAST               1 (y)

  6          10 LOAD_FAST                0 (x)
             12 LOAD_FAST                1 (y)
             14 BINARY_OP                0 (+)
             18 RETURN_VALUE
```
å½“ä½¿ç”¨ PyTorch DataLoader æ—¶ï¼Œæ¯ä¸ª worker è¿›ç¨‹éƒ½éœ€è¦ç‹¬ç«‹æ‰§è¡Œå­—èŠ‚ç ï¼Œè¿™å°±æ¶‰åŠåˆ°ä¸‹é¢çš„ GIL é—®é¢˜ã€‚

#### 1.1.3 æ·±å…¥ç†è§£ GILï¼ˆå…¨å±€è§£é‡Šå™¨é” Global Interpreter Lockï¼‰

**GILæ˜¯ä»€ä¹ˆï¼Ÿ**

GIL æ˜¯ CPython è§£é‡Šå™¨ä¸­çš„ä¸€ä¸ªæœºåˆ¶ï¼Œç”¨äºä¿æŠ¤å†…å­˜ç®¡ç†ï¼Œé˜²æ­¢å¤šä¸ªçº¿ç¨‹åŒæ—¶æ‰§è¡Œ Python å­—èŠ‚ç æ—¶å‡ºç°æ•°æ®ç«äº‰å’Œå†…å­˜ç ´åçš„é—®é¢˜ã€‚

> GILæ˜¯åœ¨å®ç°Pythonè§£æå™¨(**CPython**)æ—¶æ‰€å¼•å…¥çš„ä¸€ä¸ªæ¦‚å¿µã€‚Pythonæ˜¯ä¸€å¥—è¯­è¨€ï¼ˆè¯­æ³•ï¼‰æ ‡å‡†ï¼ŒåŒæ ·ä¸€æ®µä»£ç å¯ä»¥é€šè¿‡CPythonï¼ŒPyPyï¼ŒPsycoç­‰ä¸åŒçš„Pythonæ‰§è¡Œç¯å¢ƒæ¥æ‰§è¡Œã€‚åƒå…¶ä¸­çš„JPythonå°±æ²¡æœ‰GILã€‚ç„¶è€Œå› ä¸ºCPythonæ˜¯å¤§éƒ¨åˆ†ç¯å¢ƒä¸‹é»˜è®¤çš„Pythonæ‰§è¡Œç¯å¢ƒã€‚æ‰€ä»¥åœ¨å¾ˆå¤šäººçš„æ¦‚å¿µé‡ŒCPythonå°±æ˜¯Pythonï¼Œä¹Ÿå°±æƒ³å½“ç„¶çš„æŠŠGILå½’ç»“ä¸ºPythonè¯­è¨€çš„ç¼ºé™·ã€‚æ‰€ä»¥è¿™é‡Œè¦å…ˆæ˜ç¡®ä¸€ç‚¹ï¼š**GILå¹¶ä¸æ˜¯Pythonçš„ç‰¹æ€§ï¼ŒPythonå®Œå…¨å¯ä»¥ä¸ä¾èµ–äºGILã€‚**

**GILä¸ºä»€ä¹ˆä¼šå­˜åœ¨ï¼Ÿ**

GILçš„é—®é¢˜å…¶å®æ˜¯ç”±äºè¿‘åå‡ å¹´æ¥åº”ç”¨ç¨‹åºå’Œæ“ä½œç³»ç»Ÿé€æ­¥ä»å¤šä»»åŠ¡å•æ ¸å¿ƒæ¼”è¿›åˆ°å¤šä»»åŠ¡å¤šæ ¸å¿ƒå¯¼è‡´çš„ , åœ¨ä¸€ä¸ªå¤è€çš„å•æ ¸CPUä¸Šè°ƒåº¦å¤šä¸ªçº¿ç¨‹ä»»åŠ¡ï¼Œå¤§å®¶ç›¸äº’å…±äº«ä¸€ä¸ªå…¨å±€é”ï¼Œè°åœ¨CPUæ‰§è¡Œï¼Œè°å°±å æœ‰è¿™æŠŠé”ï¼Œç›´åˆ°è¿™ä¸ªçº¿ç¨‹å› ä¸ºIOæ“ä½œæˆ–è€…Timer Tickåˆ°æœŸè®©å‡ºCPUï¼Œæ²¡æœ‰åœ¨æ‰§è¡Œçš„çº¿ç¨‹å°±å®‰é™çš„ç­‰å¾…ç€è¿™æŠŠé”ï¼ˆé™¤äº†ç­‰å¾…ä¹‹å¤–ï¼Œä»–ä»¬åº”è¯¥ä¹Ÿæ— äº‹å¯åšï¼‰ã€‚ä¸‹é¢è¿™ä¸ªå›¾æ¼”ç¤ºäº†ä¸€ä¸ªå•æ ¸CPUçš„çº¿ç¨‹è°ƒåº¦æ–¹å¼ï¼š
![alt text](image-8.png)

ç„¶è€Œï¼Œåœ¨å¤šæ ¸CPUä¸Šï¼Œå¤šä¸ªçº¿ç¨‹å¯ä»¥åŒæ—¶åœ¨ä¸åŒçš„CPUæ ¸å¿ƒä¸Šæ‰§è¡Œï¼Œè¿™æ—¶GILå°±æˆäº†ä¸€ä¸ªç“¶é¢ˆï¼Œå› ä¸ºå®ƒé™åˆ¶äº†åŒä¸€æ—¶åˆ»åªèƒ½æœ‰ä¸€ä¸ªçº¿ç¨‹åœ¨æ‰§è¡ŒPythonå­—èŠ‚ç ï¼Œå¯¼è‡´å¤šçº¿ç¨‹æ— æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸CPUçš„ä¼˜åŠ¿ã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒPythonç¤¾åŒºå¼•å…¥äº†**å¤šè¿›ç¨‹ï¼ˆmultiprocessingï¼‰æ¨¡å—**ï¼Œé€šè¿‡åˆ›å»ºå¤šä¸ªç‹¬ç«‹çš„è¿›ç¨‹æ¥å®ç°çœŸæ­£çš„å¹¶è¡Œè®¡ç®—ï¼Œæ¯ä¸ªè¿›ç¨‹éƒ½æœ‰è‡ªå·±çš„Pythonè§£é‡Šå™¨å’ŒGILï¼Œä»è€Œç»•è¿‡äº†GILçš„é™åˆ¶ã€‚

**å¤šçº¿ç¨‹ vs å¤šè¿›ç¨‹**

åœ¨ Python ä¸­ï¼Œå¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹å„æœ‰ä¼˜ç¼ºç‚¹ï¼š
- **å¤šçº¿ç¨‹**ï¼š
  - ä¼˜ç‚¹ï¼šçº¿ç¨‹é—´åˆ‡æ¢å¼€é”€å°ï¼Œå†…å­˜å ç”¨ä½ï¼Œé€‚åˆ IO å¯†é›†å‹ä»»åŠ¡ã€‚
  - ç¼ºç‚¹ï¼šå— GIL é™åˆ¶ï¼Œæ— æ³•å……åˆ†åˆ©ç”¨å¤šæ ¸ CPUï¼Œé€‚åˆè½»é‡çº§ä»»åŠ¡ã€‚
  - é€‚ç”¨åœºæ™¯ï¼šç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶è¯»å†™ç­‰ IO å¯†é›†å‹ä»»åŠ¡ã€‚
- **å¤šè¿›ç¨‹**ï¼š
  - ä¼˜ç‚¹ï¼šæ¯ä¸ªè¿›ç¨‹æœ‰ç‹¬ç«‹çš„ GILï¼Œèƒ½å……åˆ†åˆ©ç”¨å¤šæ ¸ CPUï¼Œé€‚åˆ CPU å¯†é›†å‹ä»»åŠ¡ã€‚
  - ç¼ºç‚¹ï¼šè¿›ç¨‹é—´åˆ‡æ¢å¼€é”€å¤§ï¼Œå†…å­˜å ç”¨é«˜ï¼Œé€‚åˆé‡é‡çº§ä»»åŠ¡ã€‚
  - é€‚ç”¨åœºæ™¯ï¼šæ•°æ®å¤„ç†ã€ç§‘å­¦è®¡ç®—ç­‰ CPU å¯†é›†å‹ä»»åŠ¡ã€‚

æ ¹æ®ä»¥ä¸Šåˆ†æï¼Œæœ‰ä¸¤ä¸ªå»ºè®®ï¼š

1. åœ¨ä»¥IOæ“ä½œä¸ºä¸»çš„IOå¯†é›†å‹åº”ç”¨ä¸­ï¼Œå¤šçº¿ç¨‹å’Œå¤šè¿›ç¨‹çš„æ€§èƒ½åŒºåˆ«å¹¶ä¸å¤§ï¼ŒåŸå› åœ¨äºå³ä½¿åœ¨Pythonä¸­æœ‰GILé”çš„å­˜åœ¨ï¼Œç”±äºçº¿ç¨‹ä¸­çš„IOæ“ä½œä¼šä½¿å¾—çº¿ç¨‹ç«‹å³é‡Šæ”¾GILï¼Œåˆ‡æ¢åˆ°å…¶ä»–éIOçº¿ç¨‹ç»§ç»­æ“ä½œï¼Œæé«˜ç¨‹åºæ‰§è¡Œæ•ˆç‡ã€‚ç›¸æ¯”è¿›ç¨‹æ“ä½œï¼Œçº¿ç¨‹æ“ä½œæ›´åŠ è½»é‡çº§ï¼Œçº¿ç¨‹ä¹‹é—´çš„é€šè®¯å¤æ‚åº¦æ›´ä½ï¼Œå»ºè®®ä½¿ç”¨å¤šçº¿ç¨‹ã€‚

2. å¦‚æœæ˜¯è®¡ç®—å¯†é›†å‹çš„åº”ç”¨ï¼Œå°½é‡ä½¿ç”¨å¤šè¿›ç¨‹æˆ–è€…åç¨‹æ¥ä»£æ›¿å¤šçº¿ç¨‹ã€‚



**GILçš„å®é™…å½±å“ç¤ºä¾‹**
```python
import threading
import time

def cpu_intensive():
    """CPU å¯†é›†å‹ä»»åŠ¡"""
    total = 0
    for i in range(10**8):
        total += i
    return total

if __name__ == '__main__':
    # å¤šçº¿ç¨‹ç‰ˆæœ¬ - åè€Œæ›´æ…¢ï¼ˆå› ä¸º GILï¼‰
    start = time.time()
    t1 = threading.Thread(target=cpu_intensive)
    t2 = threading.Thread(target=cpu_intensive)
    t1.start()
    t2.start()
    t1.join()
    t2.join()
    print(f"å¤šçº¿ç¨‹è€—æ—¶: {time.time() - start:.2f}s")  

    # å¤šè¿›ç¨‹ç‰ˆæœ¬ - çœŸæ­£å¹¶è¡Œ
    from multiprocessing import Process
    start = time.time()
    p1 = Process(target=cpu_intensive)
    p2 = Process(target=cpu_intensive)
    p1.start()
    p2.start()
    p1.join()
    p2.join()
    print(f"å¤šè¿›ç¨‹è€—æ—¶: {time.time() - start:.2f}s")  
```
å¤šçº¿ç¨‹è€—æ—¶: 3.38s
å¤šè¿›ç¨‹è€—æ—¶: 1.65s


**ä¸ºä»€ä¹ˆ AI ä»£ç ä¸­å¸¸å¸¸ç”¨å¤šè¿›ç¨‹è€Œä¸æ˜¯å¤šçº¿ç¨‹ï¼Ÿ**

å› ä¸ºï¼š
- æ•°æ®åŠ è½½ = CPU å¯†é›†å‹ï¼ˆæ•°æ®å¢å¼ºã€é¢„å¤„ç†éƒ½æ˜¯ CPU æ“ä½œï¼‰
- PyTorch DataLoader é»˜è®¤ç”¨ `num_workers > 0` æ—¶ï¼Œä½¿ç”¨å¤šè¿›ç¨‹åŠ è½½æ•°æ®ï¼Œç»•è¿‡ GIL é™åˆ¶ï¼Œå®ç°çœŸæ­£å¹¶è¡Œã€‚


### 1.2 å˜é‡ã€ä½œç”¨åŸŸä¸å†…å­˜ç®¡ç†

#### 1.2.1 Pythonå¼•ç”¨è®¡æ•°æœºåˆ¶
åœ¨pythonä¸­çš„åƒåœ¾å›æ”¶æœºåˆ¶ä¸»è¦æ˜¯ä»¥å¼•ç”¨è®¡æ•°ä¸ºä¸»è¦æ‰‹æ®µä»¥æ ‡è®°æ¸…é™¤å’Œéš”ä»£å›æ”¶æœºåˆ¶ä¸ºè¾…çš„æ‰‹æ®µ ã€‚å¯ä»¥å¯¹å†…å­˜ä¸­æ— æ•ˆæ•°æ®çš„è‡ªåŠ¨ç®¡ç†ï¼Œæ€ä¹ˆçŸ¥é“ä¸€ä¸ªå¯¹è±¡èƒ½ä¸èƒ½è¢«è°ƒç”¨äº†å‘¢ï¼Ÿ

**å›é¡¾å†…å­˜åœ°å€**

Pythonä¸­çš„ä»»ä½•å˜é‡éƒ½æœ‰å¯¹åº”çš„å†…å­˜å¼•ç”¨ï¼Œä¹Ÿå°±æ˜¯å†…å­˜åœ°å€ã€‚å¦‚æœä¸æ˜¯å®¹å™¨ç±»å‹ï¼Œé‚£ä¹ˆç›´æ¥å¼•ç”¨å’Œèµ‹å€¼ï¼Œå†…å­˜åœ°å€éƒ½æ˜¯ä¸ä¼šçš„ã€‚

```
>>> a = 1
>>> b = 1
>>> id(a)
140709385600544
>>> id(b)
140709385600544
```
å¦‚æœåœ¨å†…å­˜ä¸­åˆ›å»ºäº†ä¸€ä¸ªlistå¯¹è±¡ï¼ˆå®¹å™¨ï¼‰ï¼Œè€Œä¸”å¯¹è¯¥å¯¹è±¡è¿›è¡Œäº†å¼•ç”¨ã€‚é‚£ä¹ˆb = [1,2]å’Œc = aæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
```
>>> a = [1,2]
>>> b = [1,2]
>>> id(a)
1966828025736
>>> id(b)
1966828044488
>>> c = a
>>> id(c)
1966828025736
```
é¦–å…ˆåœ¨å†…å­˜1966828025736å¤„åˆ›å»ºäº†ä¸€ä¸ªåˆ—è¡¨ [1,2]ï¼Œç„¶åå®šä¹‰äº†ä¸€ä¸ªåä¸ºaçš„å˜é‡ã€‚b = [1,2]ä¼šæ–°å¼€ä¸€ä¸ªå†…å­˜åœ°å€ï¼Œc = aç›´æ¥èµ‹å€¼ç›´æ¥å¼•ç”¨[1,2]çš„å†…å­˜åœ°å€ã€‚

**å¼•ç”¨è®¡æ•°**

åœ¨ä¸€äº›ä»£ç ä¸­ï¼Œå¦‚æœå­˜åœ¨ä¸€äº›å˜é‡ä½†æ˜¯æ²¡æœ‰ç”¨ï¼Œä¼šé€ æˆå†…å­˜ç©ºé—´ï¼Œå› æ­¤å«åšåƒåœ¾ï¼Œæ‰€ä»¥è¦å›æ”¶ã€‚

å¼•ç”¨è®¡æ•°ä¹Ÿæ˜¯ä¸€ç§æœ€ç›´è§‚ï¼Œæœ€ç®€å•çš„åƒåœ¾æ”¶é›†æŠ€æœ¯ã€‚åŸç†éå¸¸ç®€å•ï¼Œæ¯ä¸€ä¸ªå¯¹è±¡éƒ½åŒ…å«äº†ä¸¤ä¸ªå¤´éƒ¨ä¿¡æ¯ï¼Œä¸€ä¸ªæ˜¯ç±»å‹æ ‡å¿—ç¬¦ï¼Œæ ‡è¯†è¿™ä¸ªå¯¹è±¡çš„ç±»å‹ï¼›å¦ä¸€ä¸ªæ˜¯è®¡æ•°å™¨ï¼Œè®°å½•å½“å‰æŒ‡å‘è¯¥å¯¹è±¡çš„å¼•ç”¨æ•°ç›®ï¼Œè¡¨ç¤ºè¿™ä¸ªå¯¹è±¡è¢«å¤šå°‘ä¸ªå˜é‡åæ‰€å¼•ç”¨ã€‚

CPython ä½¿ç”¨å¼•ç”¨è®¡æ•°æ¥ç®¡ç†å†…å­˜ï¼Œæ‰€æœ‰ Python è„šæœ¬ä¸­åˆ›å»ºçš„å®ä¾‹ï¼Œéƒ½ä¼šæœ‰ä¸€ä¸ªå¼•ç”¨è®¡æ•°ï¼Œæ¥è®°å½•æœ‰å¤šå°‘ä¸ªæŒ‡é’ˆæŒ‡å‘å®ƒã€‚å½“å¼•ç”¨è®¡æ•°åªæœ‰ 0 æ—¶ï¼Œåˆ™ä¼šè‡ªåŠ¨é‡Šæ”¾å†…å­˜ã€‚

åœ¨Pythonä¸­é€šè¿‡sys.getrefcountæŸ¥çœ‹å¼•ç”¨è®¡æ•°çš„æ–¹æ³•ï¼š

```python
print(sys.getrefcount())
```

**æ³¨æ„:è°ƒç”¨getrefcount()å‡½æ•°ä¼šä¸´æ—¶å¢åŠ ä¸€æ¬¡å¼•ç”¨è®¡æ•°ï¼Œå¾—åˆ°çš„ç»“æœæ¯”é¢„æœŸçš„å¤šä¸€æ¬¡ã€‚**

æ¯”å¦‚ï¼Œä¸‹é¢è¿™ä¸ªä¾‹å­ä¸­ï¼Œa çš„å¼•ç”¨è®¡æ•°æ˜¯ 3ï¼Œå› ä¸ºæœ‰ aã€b å’Œä½œä¸ºå‚æ•°ä¼ é€’çš„ getrefcount è¿™ä¸‰ä¸ªåœ°æ–¹ï¼Œéƒ½å¼•ç”¨äº†ä¸€ä¸ªç©ºåˆ—è¡¨ã€‚

```
>>> import sys
>>> a = []
>>> b = a
>>> print(sys.getrefcount(a))
3
```
Python ç”¨å¼•ç”¨è®¡æ•°æ¥ç®¡ç†å†…å­˜ï¼š
- æ¯ä¸ªå¯¹è±¡éƒ½æœ‰ä¸€ä¸ª `refcount`ï¼ˆå¼•ç”¨è®¡æ•°ï¼‰
- å½“ refcount = 0 æ—¶ï¼Œåƒåœ¾å›æ”¶å™¨ç«‹å³é‡Šæ”¾å†…å­˜

```python
import sys

x = [1, 2, 3]
print(sys.getrefcount(x))  # è‡³å°‘ 2ï¼ˆx çš„å¼•ç”¨ + getrefcount å‚æ•°çš„å¼•ç”¨ï¼‰

y = x  # å¼•ç”¨è®¡æ•° +1
print(sys.getrefcount(x))  # ç°åœ¨æ˜¯ 3

del y  # å¼•ç”¨è®¡æ•° -1
print(sys.getrefcount(x))  # å›åˆ° 2
```

**è®¡æ•°å¢åŠ å’Œå‡å°‘**

ä¸‹é¢å¼•ç”¨è®¡æ•°å¢åŠ çš„åœºæ™¯ï¼š

+ å¯¹è±¡è¢«åˆ›å»ºå¹¶èµ‹å€¼ç»™æŸä¸ªå˜é‡ï¼Œæ¯”å¦‚ï¼ša = 'ABC'
+ å˜é‡é—´çš„ç›¸äº’å¼•ç”¨ï¼ˆç›¸å½“äºå˜é‡æŒ‡å‘äº†åŒä¸€ä¸ªå¯¹è±¡ï¼‰ï¼Œæ¯”å¦‚ï¼šb=a
+ å˜é‡ä½œä¸ºå‚æ•°ä¼ åˆ°å‡½æ•°ä¸­ã€‚æ¯”å¦‚ï¼šref_method(a)ï¼Œ
+ å°†å¯¹è±¡æ”¾åˆ°æŸä¸ªå®¹å™¨å¯¹è±¡ä¸­(åˆ—è¡¨ã€å…ƒç»„ã€å­—å…¸)ã€‚æ¯”å¦‚ï¼šc = [1, a, 'abc']

å¼•ç”¨è®¡æ•°å‡å°‘çš„åœºæ™¯ï¼š

+ å½“ä¸€ä¸ªå˜é‡ç¦»å¼€äº†ä½œç”¨åŸŸï¼Œæ¯”å¦‚ï¼šå‡½æ•°æ‰§è¡Œå®Œæˆæ—¶ï¼Œæ‰§è¡Œæ–¹æ³•å‰åçš„å¼•ç”¨è®¡æ•°ä¿æŒä¸å˜ï¼Œè¿™å°±æ˜¯å› ä¸ºæ–¹æ³•æ‰§è¡Œå®Œåï¼Œå¯¹è±¡çš„å¼•ç”¨è®¡æ•°ä¹Ÿä¼šå‡å°‘ï¼Œå¦‚æœåœ¨æ–¹æ³•å†…æ‰“å°ï¼Œåˆ™èƒ½çœ‹åˆ°å¼•ç”¨è®¡æ•°å¢åŠ çš„æ•ˆæœã€‚
+ å¯¹è±¡çš„å¼•ç”¨å˜é‡è¢«é”€æ¯æ—¶ï¼Œæ¯”å¦‚del aæˆ–è€…del bã€‚æ³¨æ„å¦‚æœdel aï¼Œå†å»è·å–açš„å¼•ç”¨è®¡æ•°ä¼šç›´æ¥æŠ¥é”™ã€‚
+ å¯¹è±¡è¢«ä»å®¹å™¨å¯¹è±¡ä¸­ç§»é™¤ï¼Œæ¯”å¦‚ï¼šc.remove(a)
+ ç›´æ¥å°†æ•´ä¸ªå®¹å™¨é”€æ¯ï¼Œæ¯”å¦‚ï¼šdel c
+ å¯¹è±¡çš„å¼•ç”¨è¢«èµ‹å€¼ç»™å…¶ä»–å¯¹è±¡ï¼Œç›¸å½“äºå˜é‡ä¸æŒ‡å‘ä¹‹å‰çš„å¯¹è±¡ï¼Œè€Œæ˜¯æŒ‡å‘äº†ä¸€ä¸ªæ–°çš„å¯¹è±¡ï¼Œè¿™ç§æƒ…å†µï¼Œå¼•ç”¨è®¡æ•°è‚¯å®šä¼šå‘ç”Ÿæ”¹å˜ã€‚(æ’é™¤ä¸¤ä¸ªå¯¹è±¡é»˜è®¤å¼•ç”¨è®¡ä¸€è‡´çš„åœºæ™¯)ã€‚




#### 1.2.2 å…¨å±€å˜é‡ vs é—­åŒ…å˜é‡
**å±€éƒ¨å˜é‡**
```python
def func():
    x = 10    # x æ˜¯å±€éƒ¨å˜é‡
    print(x)
 
func()
#print(x) # æŠ¥é”™ï¼šx æœªå®šä¹‰ï¼ˆè¶…å‡ºä½œç”¨åŸŸï¼‰

```
**å…¨å±€å˜é‡**
```python
name = "Alice"  #å…¨å±€å˜é‡
def greet():
    print(f"ä½ å¥½,{name}")  # å¯ä»¥è®¿é—®å…¨å±€å˜é‡
 
greet()
print(name)  #åŒæ ·å¯è®¿é—®
```
**å‡½æ•°ä¸­ä¿®æ”¹å…¨å±€å˜é‡**
```python
count = 0
 
def add():
    # count += 1 # æŠ¥é”™ï¼šUnboundLocalError,è§£é‡Šå™¨è®¤ä¸ºcountæ˜¯å±€éƒ¨å˜é‡ä½†æœªèµ‹å€¼
    print(count)

add()
```
**ä½¿ç”¨globalå…³é”®å­—ä¿®æ”¹å…¨å±€å˜é‡**
```python
count = 0
 
def add():
    global count  #å£°æ˜æˆ‘ä»¬ç›¸ç”¨çš„æ˜¯å…¨å±€å˜é‡
    conut += 1
    print("å½“å‰è®¡æ•°:",count)
 
 
add()
print(count)       #è¾“å‡ºï¼š1
```
**åµŒå¥—å‡½æ•°ä¸é—­åŒ…ï¼ˆClosureï¼‰**
ä»€ä¹ˆæ˜¯é—­åŒ…ï¼ˆClosureï¼‰ï¼Ÿ

ä¸€ä¸ªå†…éƒ¨å‡½æ•°å¼•ç”¨äº†å®ƒå¤–éƒ¨å‡½æ•°çš„å±€éƒ¨å˜é‡ï¼Œå³ä½¿å¤–éƒ¨å‡½æ•°å·²ç»æ‰§è¡Œå®Œæ¯•ï¼Œè¿™ä¸ªå†…éƒ¨å‡½æ•°ä¾ç„¶â€œè®°å¾—â€å®ƒå¤–éƒ¨çš„å˜é‡
```python
def outer():
    captured = [1, 2, 3]  # é—­åŒ…ä¸­è¢«æ•è·çš„å˜é‡
    
    def inner():
        print(captured)  # å¯è®¿é—®å¤–å±‚å˜é‡
        local_var = 99   # ä»…åœ¨ inner å†…æœ‰æ•ˆ
    
    return inner

func = outer()
# captured ä»ç„¶åœ¨å†…å­˜ä¸­ï¼Œå› ä¸ºè¢« inner å¼•ç”¨ï¼ˆé—­åŒ…ï¼‰
```
ç»“æ„ç‰¹å¾ï¼ˆä¸‰ä¸ªæ¡ä»¶ï¼‰ï¼š
1. å‡½æ•°åµŒå¥—ï¼šå‡½æ•°é‡Œé¢å®šä¹‰å‡½æ•°
2. å†…éƒ¨å‡½æ•°ä½¿ç”¨äº†å¤–éƒ¨å‡½æ•°çš„å±€éƒ¨å˜é‡
3. å¤–éƒ¨å‡½æ•°è¿”å›å†…éƒ¨å‡½æ•°

**ä¸ºä»€ä¹ˆé—­åŒ…é‡è¦ï¼Ÿ**
1. å»¶è¿Ÿæ‰§è¡Œ+è®°ä½çŠ¶æ€ï¼šé—­åŒ…å…è®¸å‡½æ•°åœ¨æœªæ¥æŸä¸ªæ—¶é—´ç‚¹æ‰§è¡Œï¼Œå¹¶ä¸”è®°ä½å½“æ—¶çš„ç¯å¢ƒçŠ¶æ€
2. ä¸æ±¡æŸ“å…¨å±€å˜é‡ï¼šç”¨å‡½æ•°å†…å˜é‡å­˜çŠ¶æ€è€Œä¸æš´éœ²å¤–éƒ¨ï¼Œé¿å…å‘½åå†²çª
3. å‡½æ•°å·¥å‚ï¼Œå›è°ƒå°è£…ï¼šé—­åŒ…å¯ä»¥åŠ¨æ€ç”Ÿæˆå‡½æ•°ï¼Œå¸¸ç”¨äºå›è°ƒã€äº‹ä»¶å¤„ç†ç­‰åœºæ™¯


è¿™åœ¨æ•°æ®å¼•å…¥ä»£ç ä¸­å¾ˆå…³é”®ï¼š
```python
def create_data_loader(data_list):
    """å·¥å‚å‡½æ•°"""
    def load_batch():
        # data_list è¢«æ•è·åœ¨é—­åŒ…ä¸­ï¼Œä¸ä¼šè¢«é‡Šæ”¾
        return data_list[:10]
    return load_batch

loader = create_data_loader([1,2,3,...,1000000])
# å³ä½¿æ•°æ®é›†å¾ˆå¤§ï¼Œåªè¦ loader è¢«å¼•ç”¨ï¼Œdata_list å°±ä¸€ç›´åœ¨å†…å­˜ä¸­
```

**é—­åŒ…å˜é‡ vs å…¨å±€å˜é‡çš„åŒºåˆ«**
![alt text](image-9.png)

![alt text](image-10.png)


#### 1.2.3 id ( ) ä¸å¯¹è±¡é©»ç•™æœºåˆ¶
**é©»ç•™æœºåˆ¶æ˜¯ä»€ä¹ˆï¼Ÿ**

é©»ç•™æœºåˆ¶æ˜¯ Python ä¸ºäº†ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæå‡æ€§èƒ½çš„ä¸€ç§ç¼“å­˜æœºåˆ¶ã€‚å¯¹äºä¸€äº›ä¸å¯å˜å¯¹è±¡ï¼ŒPython ä¼šåœ¨å†…å­˜ä¸­ä¿ç•™è¿™äº›å¯¹è±¡çš„ä¸€ä¸ªå‰¯æœ¬ï¼ˆé©»ç•™æ± ï¼‰ã€‚å½“éœ€è¦åˆ›å»ºç›¸åŒå€¼çš„å¯¹è±¡æ—¶ï¼Œç›´æ¥å¼•ç”¨å·²æœ‰çš„å¯¹è±¡ï¼Œè€Œä¸æ˜¯é‡æ–°åˆ†é…å†…å­˜ã€‚


**å…·æœ‰é©»ç•™æœºåˆ¶çš„ç±»å‹**
![alt text](image-11.png)

ç¬¦åˆæ ‡è¯†ç¬¦è§„åˆ™çš„å­—ç¬¦ä¸²ï¼šä»…ç”±å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ç»„æˆï¼Œä¸ä»¥æ•°å­—å¼€å¤´

**ä¸å…·å¤‡é©»ç•™æœºåˆ¶çš„ç±»å‹**
![alt text](image-12.png)


**æ‰‹åŠ¨é©»ç•™**

å¦‚æœå­—ç¬¦ä¸²æ˜¯é€šè¿‡è¡¨è¾¾å¼ï¼ˆå¦‚ +ã€joinï¼‰åŠ¨æ€ç”Ÿæˆçš„ï¼Œé»˜è®¤ä¸ä¼šé©»ç•™ã€‚å¯¹äºåŠ¨æ€ç”Ÿæˆçš„å­—ç¬¦ä¸²å’Œå…¶ä»–ä¸ç¬¦åˆè‡ªåŠ¨é©»ç•™æ¡ä»¶çš„å¯¹è±¡ï¼Œå¯ä»¥ä½¿ç”¨ sys.intern() æ‰‹åŠ¨é©»ç•™ã€‚ä¸ç¬¦åˆæ ‡è¯†ç¬¦è§„åˆ™çš„å­—ç¬¦ä¸²ï¼Œå¯ä»¥ä½¿ç”¨ sys.intern() å¼ºåˆ¶é©»ç•™ã€‚
```python
import sys

a = "hello, world"
b = "hello, world"
print(a is b)  # False

c = sys.intern("hello, world")
d = sys.intern("hello, world")
print(c is d)  # True
```
```python
# ä½¿ç”¨ join åŠ¨æ€ç”Ÿæˆå­—ç¬¦ä¸²
a = "".join(["hello", "world"])
b = "helloworld"
print(a, b)
print(a is b)  # Falseï¼Œå› ä¸º a æ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼Œb æ˜¯å­—é¢é‡å­—ç¬¦ä¸²
print(a == b)  # Trueï¼Œå› ä¸ºå†…å®¹ç›¸åŒ
```
å¯¹åŠ¨æ€ç”Ÿæˆçš„å­—ç¬¦ä¸²ï¼Œå¯ä»¥ä½¿ç”¨ sys.intern() å¼ºåˆ¶é©»ç•™
```python
import sys

s1 = "hello, " + "world"         # åŠ¨æ€ç”Ÿæˆ
s2 = sys.intern("hello, world")  # æ‰‹åŠ¨é©»ç•™
print(s1 is s2)                  # False
print(sys.intern(s1) is s2)      # True  
```
è¿™çœ‹ä¼¼æ— å…³ï¼Œä½†åœ¨å¤„ç†æ•°æ®å¢å¼ºæ—¶ï¼Œå¦‚æœä½ ä¸å°å¿ƒå¤šæ¬¡å¤åˆ¶äº†æ•°æ®ï¼Œå¼•ç”¨è®¡æ•°å’Œé©»ç•™æœºåˆ¶å¯èƒ½å¯¼è‡´å†…å­˜æ³„æ¼ã€‚



é¿å…æ•°æ®å¢å¼º/åŠ è½½æ—¶å‡ºç°éšæ€§å¤åˆ¶ â†’ é™ä½æ˜¾å­˜/å†…å­˜æ¶ˆè€—ã€‚ä¾‹å¦‚ï¼š

```python
# é”™è¯¯æ–¹å¼ï¼šå¤šæ¬¡å¤åˆ¶æ•°æ®
def bad_augment(image):
    img1 = image.copy()      # å¤šä½™å¤åˆ¶
    img2 = img1.copy()       # åˆå¤åˆ¶äº†
    return img2

# æ­£ç¡®æ–¹å¼ï¼šåŸåœ°æ“ä½œæˆ–å°‘å¤åˆ¶
import numpy as np
def good_augment(image):
    # ç›´æ¥ä¿®æ”¹ï¼Œå‡å°‘å†…å­˜å¼€é”€
    image = image.astype(np.float32) / 255.0
    return image
```

## 2. è¿­ä»£å™¨ä¸ç”Ÿæˆå™¨

### 2.1 è¿­ä»£å™¨åè®®

ä»»ä½•å®ç°äº† `__iter__()` å’Œ `__next__()` çš„å¯¹è±¡éƒ½æ˜¯**è¿­ä»£å™¨**ã€‚

```python
# è‡ªå®šä¹‰è¿­ä»£å™¨
class CountUp:
    def __init__(self, max):
        self.max = max
        self.current = 0
    
    def __iter__(self):
        return self  # è¿”å›è‡ªå·±
    
    def __next__(self):
        if self.current < self.max:
            self.current += 1
            return self.current
        else:
            raise StopIteration  # è¿­ä»£ç»“æŸ

# ä½¿ç”¨
for num in CountUp(3):
    print(num)  # è¾“å‡º 1, 2, 3
```

**å¯è¿­ä»£å¯¹è±¡ vs è¿­ä»£å™¨å¯¹è±¡**

- **å¯è¿­ä»£å¯¹è±¡**ï¼šå®ç°äº† `__iter__()` çš„å¯¹è±¡ï¼ˆå¦‚åˆ—è¡¨ã€å­—ç¬¦ä¸²ã€é›†åˆï¼‰
- **è¿­ä»£å™¨å¯¹è±¡**ï¼šå®ç°äº† `__iter__()` å’Œ `__next__()` çš„å¯¹è±¡

```python
# åˆ—è¡¨æ˜¯å¯è¿­ä»£çš„ï¼Œä½†ä¸æ˜¯è¿­ä»£å™¨
lst = [1, 2, 3]
print(hasattr(lst, '__iter__'))  # True
print(hasattr(lst, '__next__'))  # False

# iter() å°†å¯è¿­ä»£å¯¹è±¡è½¬ä¸ºè¿­ä»£å™¨
iterator = iter(lst)
print(hasattr(iterator, '__next__'))  # True
print(next(iterator))  # 1
print(next(iterator))  # 2
```

### 2.2 ç”Ÿæˆå™¨åŸºç¡€ï¼šä»åˆ—è¡¨åˆ°æµ

**yield çš„æ¥å†å’Œæœ¬è´¨**

åœ¨ Python 2.2 ä¹‹å‰ï¼Œå¦‚æœä½ è¦éå†å¤§æ•°æ®é›†ï¼Œå¿…é¡»ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜ä¸­ã€‚è¿™å¯¹ AI æ¥è¯´æ˜¯ç¾éš¾æ€§çš„â€”â€”æƒ³è±¡ ImageNetï¼ˆæ•°ç™¾GBï¼‰ï¼Œä½ æ— æ³•å…¨éƒ¨åŠ è½½ã€‚

ç”Ÿæˆå™¨é€šè¿‡ `yield` æä¾›äº†è§£å†³æ–¹æ¡ˆï¼š**å‡½æ•°å¯ä»¥åœ¨ä¸­é€”æš‚åœï¼Œä¿å­˜çŠ¶æ€ï¼Œç­‰å¾…ä¸‹ä¸€æ¬¡å”¤é†’**ã€‚è¿™ç§"å»¶è¿Ÿè®¡ç®—"æ€æƒ³æ˜¯ç°ä»£æ•°æ®å¤„ç†çš„åŸºç¡€ã€‚

å…·ä½“å·¥ä½œåŸç†ï¼š
- é¦–æ¬¡ `next()`ï¼šå‡½æ•°æ‰§è¡Œè‡³ `yield`ï¼Œè¿”å›å€¼ï¼Œç„¶åæš‚åœ
- å†æ¬¡ `next()`ï¼šä»æš‚åœå¤„ç»§ç»­æ‰§è¡Œï¼Œç›´åˆ°ä¸‹ä¸€ä¸ª `yield`
- å‡½æ•°è¿”å›æˆ–æŠ›å‡º `StopIteration`ï¼šè¿­ä»£ç»“æŸ

è¿™ä½¿å¾—ç”Ÿæˆå™¨æ˜¯**æœ‰çŠ¶æ€çš„è¿­ä»£å™¨**â€”â€”å®ƒè®°ä½ä¸Šæ¬¡æ‰§è¡Œåˆ°å“ªé‡Œã€‚

```python
def simple_generator():
    print("å¼€å§‹")
    yield 1
    print("ç»§ç»­")
    yield 2
    print("ç»“æŸ")
    yield 3

gen = simple_generator()
print(next(gen))  # "å¼€å§‹", è¿”å› 1
print(next(gen))  # "ç»§ç»­", è¿”å› 2
print(next(gen))  # "ç»“æŸ", è¿”å› 3
```

**ä¸ºä»€ä¹ˆç”Ÿæˆå™¨å¯¹ AI è‡³å…³é‡è¦ï¼Ÿ**

å¯¹æ¯”ä¸¤ç§æ–¹å¼ï¼š
```python
# æ–¹å¼ 1ï¼šåˆ—è¡¨ - ä¸€æ¬¡æ€§åˆ›å»º
big_list = [x**2 for x in range(10**6)]  # ç«‹å³å ç”¨ ~40MB

# æ–¹å¼ 2ï¼šç”Ÿæˆå™¨ - æŒ‰éœ€è®¡ç®—
big_gen = (x**2 for x in range(10**6))  # åªå ç”¨å‡  KB
```

åœ¨è®­ç»ƒä¸­ï¼Œç”Ÿæˆå™¨å…è®¸ä½ ï¼š
- å¤„ç†è¶…å¤§æ•°æ®é›†ï¼ˆåªåœ¨å†…å­˜ä¸­æ”¾ä¸€å°æ‰¹ï¼‰
- å®æ—¶æ•°æ®å¢å¼ºï¼ˆæ¯ä¸ª epoch çœ‹åˆ°ä¸åŒå¢å¼ºï¼‰
- æ— é™æ•°æ®æµï¼ˆOnline Learningï¼‰

PyTorch çš„ `DataLoader` å’Œ `IterableDataset` éƒ½åŸºäºç”Ÿæˆå™¨æ€æƒ³ã€‚

### 2.3 é¢å‘ AI çš„ç”Ÿæˆå™¨åº”ç”¨åœºæ™¯

**è‡ªå®šä¹‰æ•°æ®é›†åŠ è½½å™¨**

åœ¨ PyTorch ä¸­ï¼Œç»§æ‰¿ `IterableDataset` å¹¶å®ç° `__iter__()` æ–¹æ³•ã€‚å®ƒè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œæ¯” `Map-style Dataset` æ›´çµæ´»â€”â€”å¯ä»¥åŠ¨æ€å†³å®šç”Ÿæˆä»€ä¹ˆæ•°æ®ï¼Œæ— éœ€é¢„å…ˆå®šä¹‰å¤§å°ã€‚

```python
class CustomDataset:
    def __iter__(self):
        for i in range(1000):
            yield (f"sample_{i}", i)
```

ä¼˜åŠ¿ï¼š
- **å†…å­˜é«˜æ•ˆ**ï¼šä¸éœ€è¦é¢„å…ˆåŠ è½½æ‰€æœ‰æ•°æ®
- **çµæ´»æ€§**ï¼šæ ¹æ®éœ€è¦åŠ¨æ€ç”Ÿæˆï¼ˆæ•°æ®å¢å¼ºã€é‡‡æ ·ï¼‰
- **æ— é™æµ**ï¼šå¯ä»¥æ— é™ yield

**æµå¼è¯»å–å¤§æ ‡æ³¨æ–‡ä»¶ï¼ˆå¦‚ COCO annotationsï¼‰**

COCO annotations å¯èƒ½æ•°GB å¤§ã€‚ç”¨åˆ—è¡¨ä¸€æ¬¡åŠ è½½ä¼š OOMã€‚ä½†ç”Ÿæˆå™¨å¯é€è¡Œè¯»å–ï¼Œ**æ— è®ºæ–‡ä»¶å¤šå¤§ï¼Œå†…å­˜ä¸­åŒæ—¶åªæœ‰ä¸€ä¸ªæ‰¹æ¬¡**ï¼š

```python
def read_annotations_stream(filepath, batch_size=32):
    import json
    batch = []
    with open(filepath) as f:
        for line in f:
            batch.append(json.loads(line))
            if len(batch) == batch_size:
                yield batch
                batch = []
    if batch:
        yield batch
```

è¿™æ¯” `json.load(open(file))` ä¸€æ¬¡æ€§åŠ è½½èŠ‚çœ 95% ä»¥ä¸Šå†…å­˜ã€‚

**æ•°æ®å¢å¼ºä¸åœ¨çº¿å­¦ä¹ **

æœ‰äº›åœºæ™¯éœ€è¦å®æ—¶ç”Ÿæˆè®­ç»ƒæ ·æœ¬ï¼ˆå¯¹æŠ—è®­ç»ƒã€åœ¨çº¿å­¦ä¹ ï¼‰ã€‚ç”Ÿæˆå™¨å®Œç¾é€‚åº”ï¼š

```python
def online_augment_generator(base_samples):
    while True:  # æ— é™å¾ªç¯
        for sample in base_samples:
            augmented = apply_random_augmentation(sample)
            yield augmented
```

è¿™è®©æ¨¡å‹æ¯ä¸ª epoch çœ‹åˆ°ä¸åŒå¢å¼ºç‰ˆæœ¬ï¼Œæå‡æ³›åŒ–ã€‚PyTorch DataLoader + num_workers å°±æ˜¯åœ¨åå°å¤šè¿›ç¨‹è¿è¡Œè¿™æ ·çš„ç”Ÿæˆå™¨ã€‚

**ä¸ºä»€ä¹ˆç”Ÿæˆå™¨æ˜¯ AI æ•°æ®å¤„ç†çš„æ ¸å¿ƒ**

ç°ä»£æ¡†æ¶ï¼ˆPyTorchã€TensorFlowï¼‰çš„æ•°æ®åŠ è½½æœºåˆ¶éƒ½å›´ç»•ç”Ÿæˆå™¨è®¾è®¡ï¼š
1. **å†…å­˜æ•ˆç‡**ï¼šæ— éœ€ä¸€æ¬¡æ€§åŠ è½½
2. **CPU é¢„åŠ è½½**ï¼šåå°ç”Ÿæˆä¸‹ä¸€æ‰¹ï¼ŒGPU è®­ç»ƒå½“å‰æ‰¹ï¼ŒCPU-GPU å¹¶è¡Œ
3. **çµæ´»æ€§**ï¼šæ”¯æŒåŠ¨æ€å¢å¼ºã€é‡‡æ ·ã€åœ¨çº¿å­¦ä¹ 
4. **å¯æ‰©å±•**ï¼šå¤„ç† TB çº§æ•°æ®é›†

**å¯¹ AI çš„é‡è¦æ€§**ï¼š

å¤§å‹æ•°æ®é›† & æ•°æ®æµå¤„ç†çš„æ ¸å¿ƒæŠ€èƒ½ã€‚ä¾‹å¦‚åœ¨å›¾åƒåˆ†ç±»ä¸­ï¼š

```python
# PyTorch Dataset çš„æ ¸å¿ƒå°±æ˜¯è¿­ä»£å™¨
import torch.utils.data as data

class CustomImageDataset(data.Dataset):
    def __init__(self, image_paths, labels, transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
    
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        # è¿™æ˜¯ä¸€ä¸ª "å»¶è¿ŸåŠ è½½" æ¨¡å¼
        # åªåœ¨éœ€è¦æ—¶åŠ è½½å›¾åƒï¼Œè€Œä¸æ˜¯å…¨éƒ¨é¢„åŠ è½½
        image = load_image(self.image_paths[idx])
        if self.transform:
            image = self.transform(image)
        return image, self.labels[idx]
```



## 3. è£…é¥°å™¨ï¼šå‡½æ•°çš„å¢å¼º

è£…é¥°å™¨æ˜¯ Python æœ€å¼ºå¤§çš„ç‰¹æ€§ä¹‹ä¸€ï¼Œä½†ä¹Ÿæ˜¯æœ€å®¹æ˜“è¢«è¯¯ç”¨çš„ã€‚åœ¨ AI å·¥ç¨‹ä¸­ï¼Œè£…é¥°å™¨ç”¨äºè®°å½•è®­ç»ƒæ—¥å¿—ã€è‡ªåŠ¨é‡è¯•ã€æ€§èƒ½ç›‘æ§ç­‰ã€‚æŒæ¡å®ƒï¼Œèƒ½è®©ä½ çš„ä»£ç æ›´ç®€æ´ã€å¯ç»´æŠ¤ã€‚

### 3.1 è£…é¥°å™¨çš„æ ¸å¿ƒæ€æƒ³

**æ¥å†ï¼šå‡½æ•°ä½œä¸ºä¸€ç­‰å¯¹è±¡**

Python ä¸­å‡½æ•°æ˜¯ä¸€ç­‰å¯¹è±¡ï¼ˆFirst-class objectï¼‰ï¼Œæ„å‘³ç€å‡½æ•°å¯ä»¥åƒæ•°æ®ä¸€æ ·è¢«èµ‹å€¼ã€ä¼ é€’ã€è¿”å›ã€‚è¿™æ˜¯è£…é¥°å™¨çš„åŸºç¡€ï¼š

```python
def greet(name):
    return f"Hello, {name}!"

say_hello = greet          # èµ‹å€¼
result = apply_func(greet) # ä½œä¸ºå‚æ•°ä¼ é€’
```

**é—­åŒ…ä¸ *args / **kwargs**

è£…é¥°å™¨çš„æŠ€æœ¯åŸºç¡€æ˜¯**é—­åŒ…**ï¼ˆclosureï¼‰â€”â€”ä¸€ä¸ªå‡½æ•°"è®°ä½"å®ƒå¤–å±‚ä½œç”¨åŸŸçš„å˜é‡ã€‚å¦å¤–ï¼Œ`*args` å’Œ `**kwargs` å…è®¸è£…é¥°å™¨æ¥å—ä»»æ„å‚æ•°çš„å‡½æ•°ï¼š

```python
def multiplier(factor):        # è¿”å›ä¸€ä¸ª"è®°ä½" factor çš„å‡½æ•°
    def multiply(x):
        return x * factor
    return multiply

times_3 = multiplier(3)
print(times_3(10))  # 30
```

`*args` æ¥æ”¶ä»»æ„ä½ç½®å‚æ•°ï¼Œ`**kwargs` æ¥æ”¶ä»»æ„å…³é”®å­—å‚æ•°ã€‚è¿™è®©è£…é¥°å™¨èƒ½é€‚ç”¨äºä»»ä½•å‡½æ•°ç­¾åã€‚

**æ— å‚æ•°è£…é¥°å™¨ï¼šæœ€ç®€å•çš„å½¢å¼**

è£…é¥°å™¨æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œæ¥å—å¦ä¸€ä¸ªå‡½æ•°ï¼Œè¿”å›ä¸€ä¸ªå¢å¼ºç‰ˆçš„å‡½æ•°ï¼š

```python
def timer_decorator(func):
    def wrapper(*args, **kwargs):
        import time
        start = time.time()
        result = func(*args, **kwargs)
        print(f"{func.__name__} è€—æ—¶: {time.time() - start:.4f}s")
        return result
    return wrapper

@timer_decorator
def train_epoch():
    import time
    time.sleep(1)
    return "å®Œæˆ"

train_epoch()  # æ‰“å°è€—æ—¶ï¼Œç„¶åè¿”å›ç»“æœ
```

`@timer_decorator` è¿™ä¸ªè¯­æ³•ç³–ç­‰ä»·äº `train_epoch = timer_decorator(train_epoch)`ã€‚

**æœ‰å‚æ•°è£…é¥°å™¨ï¼šå¤šå±‚åµŒå¥—**

æœ‰æ—¶ä½ éœ€è¦ç»™è£…é¥°å™¨ä¼ å‚ã€‚è¿™éœ€è¦å¤šä¸€å±‚å‡½æ•°åµŒå¥—ï¼š

```python
def repeat_decorator(times):     # ç¬¬ä¸€å±‚ï¼šè£…é¥°å™¨å·¥å‚
    def decorator(func):         # ç¬¬äºŒå±‚ï¼šè£…é¥°å™¨
        def wrapper(*args, **kwargs):  # ç¬¬ä¸‰å±‚ï¼šåŒ…è£…å™¨
            for _ in range(times):
                func(*args, **kwargs)
        return wrapper
    return decorator

@repeat_decorator(times=3)
def predict():
    print("é¢„æµ‹ä¸­...")

predict()  # æ‰“å°ä¸‰é"é¢„æµ‹ä¸­..."
```

ç†è§£è¿™ä¸‰å±‚åµŒå¥—å¾ˆå…³é”®â€”â€”å®ƒå…è®¸ä½ è‡ªå®šä¹‰è£…é¥°å™¨çš„è¡Œä¸ºã€‚

### 3.2 è£…é¥°å™¨çš„å·¥ç¨‹å®è·µ

**functools.wrapsï¼šä¿æŒå…ƒä¿¡æ¯**

å½“ä½ è£…é¥°ä¸€ä¸ªå‡½æ•°æ—¶ï¼Œè¢«åŒ…è£…å‡½æ•°çš„ `__name__`ã€`__doc__` ç­‰å…ƒä¿¡æ¯ä¼šä¸¢å¤±ï¼Œå˜æˆ `wrapper`ã€‚è¿™åœ¨è°ƒè¯•æ—¶å¾ˆå›°æ‰°ã€‚`functools.wraps` è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼š

```python
from functools import wraps

def good_decorator(func):
    @wraps(func)  # å¤åˆ¶åŸå‡½æ•°çš„å…ƒä¿¡æ¯
    def wrapper(*args, **kwargs):
        return func(*args, **kwargs)
    return wrapper

@good_decorator
def important_function():
    """è¿™æ˜¯é‡è¦å‡½æ•°"""
    pass

print(important_function.__name__)  # ä¿ç•™åŸå
print(important_function.__doc__)   # ä¿ç•™æ–‡æ¡£
```

**ç¼“å­˜è£…é¥°å™¨ï¼šé¿å…é‡å¤è®¡ç®—**

å¦‚æœæŸä¸ªå‡½æ•°åœ¨ç›¸åŒè¾“å…¥ä¸‹æ€»æ˜¯è¿”å›ç›¸åŒç»“æœï¼Œå¯ä»¥ç¼“å­˜ç»“æœï¼š

```python
from functools import lru_cache

@lru_cache(maxsize=128)
def expensive_function(n):
    import time
    time.sleep(1)  # æ¨¡æ‹Ÿæ˜‚è´µè®¡ç®—
    return n * n

result1 = expensive_function(5)  # è€—æ—¶ 1 ç§’ï¼Œè®¡ç®—å¹¶ç¼“å­˜
result2 = expensive_function(5)  # ç«‹å³è¿”å›ï¼ˆä»ç¼“å­˜ï¼‰
```

åœ¨ AI æ¨ç†ä¸­ï¼Œè¿™å¾ˆæœ‰ç”¨â€”â€”é¿å…é‡å¤æ¨ç†ç›¸åŒè¾“å…¥ã€‚

### 3.3 AI å·¥ç¨‹ä¸­çš„è£…é¥°å™¨åº”ç”¨

**è®­ç»ƒè¿‡ç¨‹çš„è®¡æ—¶ä¸ç›‘æ§**

åœ¨æ·±åº¦å­¦ä¹ ä¸­ï¼Œä½ éœ€è¦ç›‘æ§å„éƒ¨åˆ†çš„è€—æ—¶ï¼ˆåŠ è½½æ•°æ®ã€å‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ç­‰ï¼‰ã€‚è£…é¥°å™¨å¯ä»¥è‡ªåŠ¨åŒ–è¿™ä¸ªè¿‡ç¨‹ï¼Œè€Œæ— éœ€åœ¨å‡½æ•°å†…éƒ¨æ•£å¸ƒè®¡æ—¶ä»£ç ï¼š

```python
def training_timer(func):
    from functools import wraps
    import time
    
    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        print(f"[{func.__name__}] è€—æ—¶: {elapsed:.3f}s")
        return result
    return wrapper

@training_timer
def load_batch(dataloader, batch_idx):
    # åŠ è½½æ•°æ®é€»è¾‘
    return batch

@training_timer
def train_step(model, batch):
    # è®­ç»ƒé€»è¾‘
    return loss
```

è¿™è®©ä½ å¯ä»¥è½»æ¾æ·»åŠ /ç§»é™¤ç›‘æ§ï¼Œè€Œæ— éœ€ä¿®æ”¹å‡½æ•°æœ¬èº«ã€‚

**è‡ªåŠ¨é‡è¯•è£…é¥°å™¨ï¼šå®¹é”™èƒ½åŠ›**

ç½‘ç»œè¯·æ±‚ã€æ•°æ®åŠ è½½ç­‰æ“ä½œå¯èƒ½å¤±è´¥ã€‚è£…é¥°å™¨å¯ä»¥è‡ªåŠ¨é‡è¯•ï¼š

```python
def retry(max_attempts=3, delay=1):
    def decorator(func):
        from functools import wraps
        import time
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts:
                        raise
                    print(f"Attempt {attempt} failed, retrying in {delay}s...")
                    time.sleep(delay)
        return wrapper
    return decorator

@retry(max_attempts=3, delay=2)
def download_dataset(url):
    # ä¸‹è½½é€»è¾‘ï¼Œå¯èƒ½å¤±è´¥
    import requests
    return requests.get(url).content
```

**æ—¥å¿—è®°å½•è£…é¥°å™¨**

è‡ªåŠ¨è®°å½•å‡½æ•°è°ƒç”¨ã€å‚æ•°å’Œè¿”å›å€¼ï¼Œå¯¹è°ƒè¯•å¾ˆæœ‰å¸®åŠ©ï¼š

```python
def log_execution(func):
    from functools import wraps
    import logging
    
    @wraps(func)
    def wrapper(*args, **kwargs):
        logging.info(f"Calling {func.__name__} with args={args}, kwargs={kwargs}")
        result = func(*args, **kwargs)
        logging.info(f"{func.__name__} returned {type(result)}")
        return result
    return wrapper

@log_execution
def predict(model, image):
    return model(image)
```

**ğŸ’¡ è£…é¥°å™¨çš„æœ€å¤§ä¼˜åŠ¿**

è£…é¥°å™¨è®©ä½ åœ¨ä¸ä¿®æ”¹åŸå‡½æ•°ä»£ç çš„æƒ…å†µä¸‹ï¼Œä¸ºå…¶æ·»åŠ æ–°åŠŸèƒ½ã€‚åœ¨å¤§å‹ AI é¡¹ç›®ä¸­ï¼Œè¿™æ„å‘³ç€ï¼š
- ä»£ç å¤ç”¨ï¼šä¸€æ¬¡å®šä¹‰ï¼Œåˆ°å¤„ä½¿ç”¨
- å…³æ³¨ç‚¹åˆ†ç¦»ï¼šåˆ†ç¦»ä¸šåŠ¡é€»è¾‘å’Œç›‘æ§/æ—¥å¿—
- æ˜“ç»´æŠ¤ï¼šä¿®æ”¹è£…é¥°å™¨æ—¶ï¼Œæ‰€æœ‰ä½¿ç”¨å®ƒçš„å‡½æ•°è‡ªåŠ¨æ›´æ–°

---

## 4. å¹¶è¡Œä¸å¹¶å‘ï¼šåŠ é€Ÿæ•°æ®å¤„ç†

**è®­ç»ƒè®¡æ—¶å™¨ decorator**

```python
from functools import wraps
import time

def training_timer(func):
    """è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­å„éƒ¨åˆ†çš„è€—æ—¶"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        print(f"ã€{func.__name__}ã€‘å¼€å§‹æ‰§è¡Œ...")
        start = time.time()
        result = func(*args, **kwargs)
        elapsed = time.time() - start
        print(f"ã€{func.__name__}ã€‘å®Œæˆï¼è€—æ—¶ {elapsed:.2f}s")
        return result
    return wrapper

@training_timer
def train_epoch():
    import time
    time.sleep(0.5)
    return "epochå®Œæˆ"

train_epoch()
# ã€train_epochã€‘å¼€å§‹æ‰§è¡Œ...
# ã€train_epochã€‘å®Œæˆï¼è€—æ—¶ 0.50s
```

**è‡ªåŠ¨æ—¥å¿—è®°å½• decorator**

```python
import logging
from functools import wraps

def auto_logger(func):
    """è‡ªåŠ¨è®°å½•å‡½æ•°çš„è¾“å…¥å’Œè¾“å‡º"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        logging.info(f"è°ƒç”¨ {func.__name__}ï¼Œå‚æ•°: {args}, {kwargs}")
        try:
            result = func(*args, **kwargs)
            logging.info(f"{func.__name__} è¿”å›: {result}")
            return result
        except Exception as e:
            logging.error(f"{func.__name__} å¼‚å¸¸: {e}")
            raise
    return wrapper

@auto_logger
def forward_pass(input_tensor):
    return "æ¨¡å‹è¾“å‡º"

forward_pass(torch.randn(1, 3, 224, 224))
```

**è‡ªåŠ¨é‡è¯•ï¼ˆå®¹é”™è®­ç»ƒä»»åŠ¡ï¼‰decorator**

```python
from functools import wraps
import time

def retry(max_attempts=3, delay=1):
    """å¤±è´¥åè‡ªåŠ¨é‡è¯•çš„è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise
                    print(f"ç¬¬ {attempt+1} æ¬¡å¤±è´¥: {e}ï¼Œ{delay}ç§’åé‡è¯•...")
                    time.sleep(delay)
        return wrapper
    return decorator

@retry(max_attempts=3, delay=2)
def download_model():
    """ä¸‹è½½æ¨¡å‹æƒé‡"""
    import random
    if random.random() < 0.7:
        raise ConnectionError("ç½‘ç»œé”™è¯¯")
    return "æ¨¡å‹åŠ è½½æˆåŠŸ"

# ä¼šè‡ªåŠ¨é‡è¯•ï¼Œç›´åˆ°æˆåŠŸæˆ–è¾¾åˆ°æœ€å¤§æ¬¡æ•°
```

**ç¼“å­˜æ¨¡å‹æ¨ç†ç»“æœï¼ˆé¿å…é‡å¤è®¡ç®—ï¼‰**

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def model_inference(input_hash):
    """ç¼“å­˜æ¨ç†ç»“æœï¼Œé¿å…é‡å¤è®¡ç®—"""
    # input_hash æ˜¯è¾“å…¥çš„å“ˆå¸Œå€¼
    # è¿™æ ·å¯ä»¥é¿å…å¯¹åŒä¸€è¾“å…¥çš„é‡å¤æ¨ç†
    return model(input_hash)

# ä½¿ç”¨ç¤ºä¾‹
image_hash = hash(str(image_array.tostring()))
result = model_inference(image_hash)  # é¦–æ¬¡è®¡ç®—
result = model_inference(image_hash)  # ç›´æ¥ä»ç¼“å­˜è¿”å›
```

**ğŸ’¡ å¯¹ AI çš„é‡è¦æ€§**ï¼š

è®­ç»ƒæµç¨‹å¯æ§ã€è‡ªåŠ¨åŒ–ã€æ¨¡å—åŒ–å†™æ³•çš„åŸºç¡€ã€‚è£…é¥°å™¨è®©ä½ çš„ä»£ç æ›´ç®€æ´ã€æ›´æ˜“ç»´æŠ¤ã€‚





---

## 4. å¹¶è¡Œ / å¹¶å‘

### 4.1 Threading å¤šçº¿ç¨‹

å¤šçº¿ç¨‹é€‚åˆ **IO å¯†é›†å‹** ä»»åŠ¡ï¼ˆå¦‚ç½‘ç»œè¯·æ±‚ã€æ–‡ä»¶ IOï¼‰ã€‚ä½†ç”±äº GILï¼Œä¸é€‚åˆ CPU å¯†é›†å‹ä»»åŠ¡ã€‚

```python
import threading
import time

def io_intensive_task(task_id):
    """æ¨¡æ‹Ÿ IO æ“ä½œï¼ˆå¦‚ä¸‹è½½æ•°æ®é›†ï¼‰"""
    print(f"ä»»åŠ¡ {task_id} å¼€å§‹")
    time.sleep(2)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
    print(f"ä»»åŠ¡ {task_id} å®Œæˆ")

# å•çº¿ç¨‹ï¼š2ä¸ªä»»åŠ¡è€—æ—¶ 4ç§’
start = time.time()
io_intensive_task(1)
io_intensive_task(2)
print(f"å•çº¿ç¨‹è€—æ—¶: {time.time() - start:.2f}s")  # 4ç§’

# å¤šçº¿ç¨‹ï¼š2ä¸ªä»»åŠ¡è€—æ—¶ 2ç§’ï¼ˆçœŸæ­£å¹¶å‘ï¼‰
start = time.time()
t1 = threading.Thread(target=io_intensive_task, args=(1,))
t2 = threading.Thread(target=io_intensive_task, args=(2,))
t1.start()
t2.start()
t1.join()  # ç­‰å¾…çº¿ç¨‹å®Œæˆ
t2.join()
print(f"å¤šçº¿ç¨‹è€—æ—¶: {time.time() - start:.2f}s")  # 2ç§’
```

### 4.2 Multiprocessingï¼ˆå¤šè¿›ç¨‹ï¼‰

å¤šè¿›ç¨‹çªç ´ GIL é™åˆ¶ï¼Œé€‚åˆ **CPU å¯†é›†å‹** ä»»åŠ¡ã€‚æ¯ä¸ªè¿›ç¨‹æœ‰ç‹¬ç«‹çš„è§£é‡Šå™¨å’Œå†…å­˜ã€‚

```python
from multiprocessing import Process, Pool
import time

def cpu_intensive_task(n):
    """CPU å¯†é›†å‹ä»»åŠ¡"""
    total = sum(i*i for i in range(n))
    return total

# å•è¿›ç¨‹ï¼šè€—æ—¶çº¦ 8ç§’
start = time.time()
cpu_intensive_task(10**8)
cpu_intensive_task(10**8)
print(f"å•è¿›ç¨‹è€—æ—¶: {time.time() - start:.2f}s")  # 8ç§’

# å¤šè¿›ç¨‹ï¼šè€—æ—¶çº¦ 4ç§’ï¼ˆçœŸæ­£å¹¶è¡Œï¼‰
start = time.time()
p1 = Process(target=cpu_intensive_task, args=(10**8,))
p2 = Process(target=cpu_intensive_task, args=(10**8,))
p1.start()
p2.start()
p1.join()
p2.join()
print(f"å¤šè¿›ç¨‹è€—æ—¶: {time.time() - start:.2f}s")  # 4ç§’
```

**è¿›ç¨‹æ± ï¼ˆPoolï¼‰**

```python
from multiprocessing import Pool

def square(x):
    return x * x

# ä½¿ç”¨è¿›ç¨‹æ± æ‰¹é‡å¤„ç†
with Pool(4) as pool:
    # map ç›¸å½“äºå†…ç½® mapï¼Œä½†åˆ†å¸ƒåœ¨ 4 ä¸ªè¿›ç¨‹ä¸Š
    results = pool.map(square, range(100))

print(results[:10])  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
```

**å…±äº«å†…å­˜ä¸æ•°æ®å¤åˆ¶**

```python
from multiprocessing import Process, Queue
import numpy as np

def worker(queue, data):
    """å­è¿›ç¨‹æ¥æ”¶æ•°æ®"""
    # âš ï¸ data è¢«å¤åˆ¶åˆ°å­è¿›ç¨‹ï¼è¿™å¾ˆè€—æ—¶
    processed = data * 2
    queue.put(processed)

# ç”¨é˜Ÿåˆ—ï¼ˆQueueï¼‰ä¼ é€’æ•°æ®
q = Queue()
large_data = np.random.randn(1000, 1000)
p = Process(target=worker, args=(q, large_data))
p.start()
result = q.get()  # ä»é˜Ÿåˆ—è·å–ç»“æœ
p.join()
```

ğŸ‘‰ **ä¸ºä»€ä¹ˆ PyTorch DataLoader é»˜è®¤ç”¨å¤šè¿›ç¨‹ï¼Ÿ**

å› ä¸ºï¼š
1. **æ•°æ®åŠ è½½ = CPU å¯†é›†å‹**ï¼ˆå›¾åƒè§£ç ã€æ•°æ®å¢å¼ºéƒ½å¾ˆè€— CPUï¼‰
2. **å¤šçº¿ç¨‹è¢« GIL å¡ä½**ï¼Œæ— æ³•çœŸæ­£å¹¶è¡Œ
3. **å¤šè¿›ç¨‹èƒ½çœŸæ­£å¹¶è¡Œ**ï¼Œå……åˆ†åˆ©ç”¨å¤šæ ¸ CPU

```python
# PyTorch DataLoader å†…éƒ¨ä½¿ç”¨å¤šè¿›ç¨‹
from torch.utils.data import DataLoader, TensorDataset
import torch

dataset = TensorDataset(torch.randn(1000, 3, 224, 224))
# num_workers=4 æ„å‘³ç€ 4 ä¸ªç‹¬ç«‹è¿›ç¨‹åŠ è½½æ•°æ®
loader = DataLoader(dataset, batch_size=32, num_workers=4)

for batch in loader:
    # èƒŒåï¼š4 ä¸ªè¿›ç¨‹åŒæ—¶åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    pass
```

### 4.3 concurrent.futuresï¼ˆé«˜çº§å¹¶è¡Œ APIï¼‰

æä¾›æ›´ç®€æ´çš„æ¥å£æ¥ç®¡ç†çº¿ç¨‹å’Œè¿›ç¨‹æ± ã€‚

```python
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import time

def task(n):
    time.sleep(1)
    return n * n

# å¤šçº¿ç¨‹æ± 
with ThreadPoolExecutor(max_workers=4) as executor:
    # submit æäº¤å•ä¸ªä»»åŠ¡
    future = executor.submit(task, 5)
    result = future.result()  # ç­‰å¾…ç»“æœ
    print(f"ç»“æœ: {result}")

# è¿›ç¨‹æ±  + map æ‰¹é‡å¤„ç†
with ProcessPoolExecutor(max_workers=4) as executor:
    results = list(executor.map(task, range(10)))
    print(results)

# Future å¯¹è±¡ç”¨äºå¼‚æ­¥ç¼–ç¨‹
futures = []
with ThreadPoolExecutor(max_workers=4) as executor:
    for i in range(10):
        future = executor.submit(task, i)
        futures.append(future)

# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
for future in futures:
    print(future.result())
```

### 4.4 Asyncioï¼ˆåç¨‹ï¼‰

```python
import asyncio

async def async_task(n):
    """å¼‚æ­¥å‡½æ•°"""
    print(f"ä»»åŠ¡ {n} å¼€å§‹")
    await asyncio.sleep(1)  # éé˜»å¡ç­‰å¾…
    print(f"ä»»åŠ¡ {n} å®Œæˆ")
    return n * n

# è¿è¡Œåç¨‹
async def main():
    # å¹¶å‘è¿è¡Œå¤šä¸ªåç¨‹
    tasks = [async_task(i) for i in range(5)]
    results = await asyncio.gather(*tasks)
    return results

results = asyncio.run(main())
print(results)
```

**ğŸ’¡ å¯¹ AI çš„é‡è¦æ€§**ï¼š

å¼‚æ­¥ç¼–ç¨‹é€‚åˆ **IO å¯†é›†ä¸”éœ€è¦é«˜åå** çš„åœºæ™¯ï¼ˆå¦‚å¼‚æ­¥æ•°æ®é¢„å¤„ç†ã€å¼‚æ­¥æ¨ç†æœåŠ¡ï¼‰ã€‚

---

## 5. æ€§èƒ½ä¼˜åŒ–ï¼ˆAI å·¥ç¨‹éå¸¸å…³é”®ï¼‰

### 5.1 å†…å­˜ä¼˜åŒ–

**æ·±æ‹·è´ vs æµ…æ‹·è´**

```python
import copy

# æµ…æ‹·è´ï¼šåªå¤åˆ¶ä¸€å±‚
original = [[1, 2], [3, 4]]
shallow = copy.copy(original)
shallow[0][0] = 999
print(original)  # [[999, 2], [3, 4]] åŸåˆ—è¡¨è¢«ä¿®æ”¹äº†ï¼

# æ·±æ‹·è´ï¼šå®Œå…¨ç‹¬ç«‹å¤åˆ¶
original = [[1, 2], [3, 4]]
deep = copy.deepcopy(original)
deep[0][0] = 999
print(original)  # [[1, 2], [3, 4]] åŸåˆ—è¡¨æœªè¢«ä¿®æ”¹
```

åœ¨æ•°æ®å¢å¼ºä¸­çš„åº”ç”¨ï¼š

```python
import numpy as np

def bad_augment(images):
    """æµ…æ‹·è´å¯¼è‡´åŸæ•°æ®è¢«ä¿®æ”¹"""
    batch = images.copy()  # æµ…æ‹·è´
    batch[:, 0] = 0  # è®¾ç½®ç¬¬ä¸€åˆ—ä¸º 0
    return batch

images = np.random.randn(32, 3, 224, 224)
augmented = bad_augment(images)
# å¦‚æœå®é™…å·¥ä½œä¸­éœ€è¦åŸæ•°æ®ï¼Œè¿™ä¼šå¯¼è‡´é—®é¢˜
```

**é¿å… numpy â†’ python åˆ—è¡¨å¤§é‡è½¬æ¢**

```python
# âŒ ä½æ•ˆï¼šè½¬æ¢ä¸º Python åˆ—è¡¨
import numpy as np
data = np.random.randn(1000000)
python_list = data.tolist()  # è½¬æ¢ï¼Œå¾ˆæ…¢
for x in python_list:
    process(x)

# âœ… é«˜æ•ˆï¼šç›´æ¥è¿­ä»£ numpy
for x in data:
    process(x)

# âœ… æ›´é«˜æ•ˆï¼šå‘é‡åŒ–æ“ä½œ
result = np.vectorize(process)(data)
```

**ä½¿ç”¨ç”Ÿæˆå™¨æ›¿ä»£åˆ—è¡¨**

```python
# âŒ ä½æ•ˆï¼šä¸€æ¬¡æ€§åŠ è½½æ•´ä¸ªæ•°æ®é›†
def load_images_bad():
    images = []
    for file in file_list:
        images.append(load_image(file))
    return images  # è¿”å›å¤§åˆ—è¡¨ï¼Œå ç”¨å¤§é‡å†…å­˜

# âœ… é«˜æ•ˆï¼šå»¶è¿ŸåŠ è½½
def load_images_good():
    for file in file_list:
        yield load_image(file)  # ç”¨ç”Ÿæˆå™¨ï¼Œå†…å­˜é«˜æ•ˆ
```

### 5.2 åŠ é€ŸæŠ€å·§

**å‘é‡åŒ–æ“ä½œ**

```python
import numpy as np
import time

# âŒ å¾ªç¯ç‰ˆæœ¬ï¼šæ…¢
def slow_sum(arr):
    total = 0
    for x in arr:
        total += x
    return total

# âœ… å‘é‡åŒ–ç‰ˆæœ¬ï¼šå¿«
def fast_sum(arr):
    return np.sum(arr)

arr = np.random.randn(10**7)

start = time.time()
slow_sum(arr)
print(f"å¾ªç¯ç‰ˆæœ¬è€—æ—¶: {time.time()-start:.4f}s")  # ~0.5s

start = time.time()
fast_sum(arr)
print(f"å‘é‡åŒ–ç‰ˆæœ¬è€—æ—¶: {time.time()-start:.4f}s")  # ~0.001s
```

**NumPy çš„å¹¿æ’­ä¼˜åŒ–**

```python
import numpy as np

# å¹¿æ’­è‡ªåŠ¨æ‰©å±•æ•°ç»„ç»´åº¦ï¼Œé¿å…æ˜¾å¼å¾ªç¯
a = np.random.randn(1000, 1)     # (1000, 1)
b = np.random.randn(1, 100)      # (1, 100)

# è‡ªåŠ¨å¹¿æ’­åˆ° (1000, 100)
result = a + b  # é«˜æ•ˆã€ç®€æ´

# ç­‰ä»·çš„ä½æ•ˆå†™æ³•ï¼š
result_slow = np.zeros((1000, 100))
for i in range(1000):
    for j in range(100):
        result_slow[i, j] = a[i, 0] + b[0, j]
```

**Memory Pinningï¼ˆåŠ é€Ÿ GPU å¤åˆ¶ï¼‰**

```python
import torch

# âŒ æ™®é€šå†…å­˜ â†’ GPUï¼šæ¶‰åŠ DMA è½¬ç§»
data = torch.randn(1000, 1000)  # CPU å†…å­˜
gpu_data = data.cuda()  # å¤åˆ¶åˆ° GPUï¼ˆè¾ƒæ…¢ï¼‰

# âœ… é”å®šå†…å­˜ â†’ GPUï¼šæ›´å¿«
pinned_data = torch.randn(1000, 1000, pin_memory=True)
gpu_data = pinned_data.cuda()  # å¤åˆ¶æ›´å¿«ï¼ˆDMA ä¼˜åŒ–ï¼‰

# DataLoader ä¸­ä½¿ç”¨ pin_memory
from torch.utils.data import DataLoader
loader = DataLoader(dataset, pin_memory=True)  # åŠ é€Ÿæ•°æ®è½¬ç§»
```

**é«˜æ•ˆæ–‡ä»¶ IOï¼ˆmmapï¼‰**

```python
import numpy as np

# âŒ æ™®é€šæ–¹å¼ï¼šä¸€æ¬¡æ€§åŠ è½½
large_data = np.load('huge_file.npy')  # å ç”¨å¤§é‡å†…å­˜

# âœ… mmap æ–¹å¼ï¼šè™šæ‹Ÿæ˜ å°„ï¼ŒæŒ‰éœ€åŠ è½½
large_data = np.load('huge_file.npy', mmap_mode='r')
print(large_data[0:100])  # åªåŠ è½½å‰ 100 è¡Œåˆ°å†…å­˜
```

### 5.3 ä»£ç å‰–æï¼ˆprofilerï¼‰

**timeit**

```python
import timeit

def function_to_profile():
    return sum(range(1000))

# æµ‹é‡æ‰§è¡Œæ—¶é—´
time_taken = timeit.timeit(function_to_profile, number=100000)
print(f"å¹³å‡æ—¶é—´: {time_taken/100000*1e6:.2f} Î¼s")
```

**cProfile**

```python
import cProfile

def slow_function():
    total = 0
    for i in range(10**6):
        total += i
    return total

# åˆ†æå‡½æ•°æ€§èƒ½
cProfile.run('slow_function()')
# è¾“å‡ºæ¯ä¸ªå‡½æ•°çš„è°ƒç”¨æ¬¡æ•°ã€æ‰§è¡Œæ—¶é—´ç­‰
```

**PyTorch çš„ profiler**

```python
import torch
from torch.profiler import profile, record_function

# åˆ†ææ¨¡å‹å‰å‘ä¼ æ’­
model = YourModel()
x = torch.randn(1, 3, 224, 224)

with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA]) as prof:
    with record_function("forward"):
        output = model(x)

print(prof.key_averages().table(sort_by="cpu_time_total", row_limit=10))
```

**ğŸ’¡ å¯¹ AI çš„é‡è¦æ€§**ï¼š

è®­ç»ƒé€Ÿåº¦æå‡ 20%â€“50% æ˜¯å¸¸è§çš„æ”¶ç›Šã€‚é€šè¿‡åˆ†æï¼Œä½ èƒ½å‘ç°çœŸæ­£çš„æ€§èƒ½ç“¶é¢ˆã€‚

---

## 6. Python è®¾è®¡æ¨¡å¼ï¼ˆå†™å¯ç»´æŠ¤ AI ä»£ç çš„å…³é”®ï¼‰

### 6.1 å·¥å‚æ¨¡å¼

ç”¨å·¥å‚å‡½æ•°/ç±»æ¥åˆ›å»ºå¯¹è±¡ï¼Œé¿å…ç¡¬ç¼–ç ã€‚

```python
# âŒ ç¡¬ç¼–ç åˆ›å»ºä¸åŒæ¨¡å‹
def train(model_name):
    if model_name == "resnet":
        model = ResNet()
    elif model_name == "vgg":
        model = VGG()
    elif model_name == "mobilenet":
        model = MobileNet()
    return model

# âœ… å·¥å‚æ¨¡å¼
class ModelFactory:
    models = {
        "resnet": ResNet,
        "vgg": VGG,
        "mobilenet": MobileNet,
    }
    
    @classmethod
    def create(cls, model_name):
        model_class = cls.models.get(model_name)
        if not model_class:
            raise ValueError(f"æœªçŸ¥æ¨¡å‹: {model_name}")
        return model_class()

# ä½¿ç”¨
model = ModelFactory.create("resnet")
```

**Dataset å·¥å‚**

```python
class DatasetFactory:
    datasets = {
        "imagenet": ImageNetDataset,
        "cifar10": CIFAR10Dataset,
        "coco": COCODataset,
    }
    
    @classmethod
    def create(cls, dataset_name, **kwargs):
        dataset_class = cls.datasets[dataset_name]
        return dataset_class(**kwargs)

# ä½¿ç”¨
dataset = DatasetFactory.create("cifar10", root="/data/cifar10")
```

### 6.2 ç­–ç•¥æ¨¡å¼

ä¸åŒçš„ç®—æ³•/ç­–ç•¥ç‹¬ç«‹å°è£…ï¼Œæ˜“äºåˆ‡æ¢ã€‚

```python
# å®šä¹‰ä¼˜åŒ–å™¨ç­–ç•¥
class OptimizerStrategy:
    def __call__(self, params, lr):
        raise NotImplementedError

class SGDStrategy(OptimizerStrategy):
    def __call__(self, params, lr):
        return torch.optim.SGD(params, lr=lr)

class AdamStrategy(OptimizerStrategy):
    def __call__(self, params, lr):
        return torch.optim.Adam(params, lr=lr)

class CosineAnnealingStrategy(OptimizerStrategy):
    """å­¦ä¹ ç‡ç­–ç•¥"""
    def __call__(self, optimizer, T_max):
        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max)

# ä½¿ç”¨ï¼šæ˜“äºåˆ‡æ¢ç­–ç•¥
class Trainer:
    def __init__(self, optimizer_strategy, scheduler_strategy):
        self.opt_strategy = optimizer_strategy
        self.sched_strategy = scheduler_strategy
    
    def setup(self, model, lr):
        optimizer = self.opt_strategy(model.parameters(), lr)
        scheduler = self.sched_strategy(optimizer, T_max=100)
        return optimizer, scheduler

# é…ç½®ä¸åŒç­–ç•¥
trainer = Trainer(AdamStrategy(), CosineAnnealingStrategy())
```

### 6.3 å•ä¾‹æ¨¡å¼

ç¡®ä¿å…¨å±€åªæœ‰ä¸€ä¸ªå®ä¾‹ï¼ˆå¦‚æ—¥å¿—ã€é…ç½®ï¼‰ã€‚

```python
class Logger:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

# ä½¿ç”¨
logger1 = Logger()
logger2 = Logger()
print(logger1 is logger2)  # True

# æ›´ç®€æ´çš„å•ä¾‹ï¼šè£…é¥°å™¨
def singleton(cls):
    instances = {}
    def get_instance(*args, **kwargs):
        if cls not in instances:
            instances[cls] = cls(*args, **kwargs)
        return instances[cls]
    return get_instance

@singleton
class Config:
    def __init__(self):
        self.lr = 0.001

cfg1 = Config()
cfg2 = Config()
print(cfg1 is cfg2)  # True
```

**ğŸ’¡ å¯¹ AI çš„é‡è¦æ€§**ï¼š

å¤§é¡¹ç›®ï¼ˆCV / NLP / RLï¼‰å¿…é¡»ä¿è¯ä»£ç å¯æ‰©å±•æ€§ã€‚è®¾è®¡æ¨¡å¼è®©ä»£ç æ›´çµæ´»ã€æ˜“ç»´æŠ¤ã€‚

---

## 7. AI å·¥ç¨‹ä¸­çš„ Python å®æˆ˜èƒ½åŠ›

### 7.1 Dataset / DataLoader è‡ªå®šä¹‰

**è‡ªå®šä¹‰è¿­ä»£å™¨**

```python
import torch
from torch.utils.data import Dataset, DataLoader

class CustomDataset(Dataset):
    def __init__(self, file_list, transform=None):
        self.file_list = file_list
        self.transform = transform
    
    def __len__(self):
        return len(self.file_list)
    
    def __getitem__(self, idx):
        # å»¶è¿ŸåŠ è½½ï¼šåªåœ¨éœ€è¦æ—¶åŠ è½½å›¾åƒ
        image = load_image(self.file_list[idx])
        if self.transform:
            image = self.transform(image)
        return image

# ä½¿ç”¨
dataset = CustomDataset(file_list)
loader = DataLoader(dataset, batch_size=32, num_workers=4)
for images in loader:
    # å¤šè¿›ç¨‹åŠ è½½æ•°æ®
    pass
```

**å¤šè¿›ç¨‹åŠ è½½**

```python
# num_workers > 0 ä¼šå¯ç”¨å¤šè¿›ç¨‹
loader = DataLoader(
    dataset,
    batch_size=32,
    num_workers=4,      # 4 ä¸ªè¿›ç¨‹
    pin_memory=True,    # é”å®šå†…å­˜åŠ é€Ÿ GPU è½¬ç§»
    prefetch_factor=2,  # é¢„åŠ è½½å› å­
)

# æ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹è°ƒç”¨ __getitem__
```

**åŠ¨æ€æ•°æ®å¢å¼º**

```python
from torchvision import transforms

# å®šä¹‰å¢å¼ºç­–ç•¥
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
])

class AugmentedDataset(Dataset):
    def __init__(self, images, labels):
        self.images = images
        self.labels = labels
        self.transform = transform
    
    def __getitem__(self, idx):
        image = self.images[idx]
        # æ¯æ¬¡è¿”å›ä¸åŒçš„å¢å¼ºç‰ˆæœ¬
        image = self.transform(image)
        return image, self.labels[idx]
```

### 7.2 è®­ç»ƒæ¡†æ¶å°è£…

**è£…é¥°å™¨è®°å½•è®­ç»ƒè¿‡ç¨‹**

```python
from functools import wraps
import logging

def log_training(func):
    """è®°å½•è®­ç»ƒè¿‡ç¨‹"""
    @wraps(func)
    def wrapper(self, *args, **kwargs):
        logging.info(f"ã€å¼€å§‹ã€‘{func.__name__}")
        result = func(self, *args, **kwargs)
        logging.info(f"ã€å®Œæˆã€‘{func.__name__}")
        return result
    return wrapper

class Trainer:
    @log_training
    def train_epoch(self):
        # è®­ç»ƒä»£ç 
        pass
    
    @log_training
    def validate(self):
        # éªŒè¯ä»£ç 
        pass
```

**å¼‚æ­¥æ•°æ®é¢„å¤„ç†**

```python
from concurrent.futures import ThreadPoolExecutor

class AsyncDataLoader:
    def __init__(self, dataset, batch_size=32, num_workers=4):
        self.dataset = dataset
        self.batch_size = batch_size
        self.executor = ThreadPoolExecutor(max_workers=num_workers)
    
    def __iter__(self):
        for i in range(0, len(self.dataset), self.batch_size):
            # æäº¤é¢„å¤„ç†ä»»åŠ¡
            future = self.executor.submit(
                self._load_batch,
                i, i + self.batch_size
            )
            yield future.result()
    
    def _load_batch(self, start, end):
        batch = [self.dataset[i] for i in range(start, end)]
        return batch
```

**æ¨¡å‹è‡ªåŠ¨ä¿å­˜/æ¢å¤**

```python
import torch
import os

class CheckpointManager:
    def __init__(self, save_dir):
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)
    
    def save(self, model, optimizer, epoch, metrics):
        path = os.path.join(self.save_dir, f"checkpoint_epoch_{epoch}.pt")
        torch.save({
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'epoch': epoch,
            'metrics': metrics,
        }, path)
        print(f"ä¿å­˜æ£€æŸ¥ç‚¹: {path}")
    
    def load(self, model, optimizer, path):
        checkpoint = torch.load(path)
        model.load_state_dict(checkpoint['model'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        epoch = checkpoint['epoch']
        print(f"åŠ è½½æ£€æŸ¥ç‚¹: {path}ï¼Œæ¢å¤åˆ° epoch {epoch}")
        return epoch

# ä½¿ç”¨
ckpt_mgr = CheckpointManager('./checkpoints')
ckpt_mgr.save(model, optimizer, epoch=10, metrics={'acc': 0.95})
```

### 7.3 æ¨¡å‹éƒ¨ç½²

**TorchScript**

```python
import torch

class SimpleModel(torch.nn.Module):
    def forward(self, x):
        return x * 2

model = SimpleModel()
# è½¬ä¸º TorchScriptï¼ˆå¯è„±ç¦» Python ç¯å¢ƒè¿è¡Œï¼‰
scripted = torch.jit.script(model)
scripted.save('model.pt')

# åŠ è½½å¹¶æ¨ç†
loaded_model = torch.jit.load('model.pt')
output = loaded_model(torch.randn(1, 10))
```

**å¤šè¿›ç¨‹æ¨ç†æœåŠ¡**

```python
from multiprocessing import Process, Queue
import torch

class InferenceWorker:
    def __init__(self, model_path, input_queue, output_queue):
        self.model = torch.jit.load(model_path)
        self.input_queue = input_queue
        self.output_queue = output_queue
    
    def run(self):
        while True:
            request_id, input_data = self.input_queue.get()
            output = self.model(input_data)
            self.output_queue.put((request_id, output))

# å¯åŠ¨å¤šä¸ªæ¨ç†è¿›ç¨‹
input_q = Queue()
output_q = Queue()
workers = [
    InferenceWorker('model.pt', input_q, output_q)
    for _ in range(4)
]
for w in workers:
    p = Process(target=w.run)
    p.start()

# æäº¤æ¨ç†è¯·æ±‚
input_q.put(('req_1', torch.randn(1, 3, 224, 224)))
request_id, output = output_q.get()
```

**å¼‚æ­¥ API æ¨ç†æœåŠ¡**

```python
import asyncio
import torch
from aiohttp import web

class AsyncInferenceServer:
    def __init__(self, model_path):
        self.model = torch.jit.load(model_path)
    
    async def infer(self, request):
        data = await request.json()
        input_tensor = torch.tensor(data['input'])
        
        # å¼‚æ­¥æ¨ç†ï¼ˆä¸é˜»å¡å…¶ä»–è¯·æ±‚ï¼‰
        output = await asyncio.to_thread(
            self._sync_infer,
            input_tensor
        )
        
        return web.json_response({'output': output.tolist()})
    
    def _sync_infer(self, x):
        return self.model(x)

# å¯åŠ¨æœåŠ¡
app = web.Application()
server = AsyncInferenceServer('model.pt')
app.router.add_post('/infer', server.infer)
web.run_app(app, port=8080)

# å®¢æˆ·ç«¯è¯·æ±‚
# curl -X POST http://localhost:8080/infer -d '{"input": [1, 2, 3]}'
```

---

## æ€»ç»“

| ä¸»é¢˜ | æ ¸å¿ƒæ¦‚å¿µ | AI åº”ç”¨åœºæ™¯ |
|------|--------|----------|
| **æ‰§è¡Œæ¨¡å‹** | GILã€å¼•ç”¨è®¡æ•° | ç†è§£ä¸ºä»€ä¹ˆ DataLoader ç”¨å¤šè¿›ç¨‹ |
| **è¿­ä»£å™¨/ç”Ÿæˆå™¨** | yieldã€å»¶è¿Ÿè®¡ç®— | é«˜æ•ˆåŠ è½½å¤§æ•°æ®é›† |
| **è£…é¥°å™¨** | é—­åŒ…ã€å…ƒç¼–ç¨‹ | è®­ç»ƒæ—¥å¿—ã€æ€§èƒ½ç›‘æ§ã€è‡ªåŠ¨é‡è¯• |
| **å¹¶è¡Œ/å¹¶å‘** | å¤šè¿›ç¨‹ã€å¼‚æ­¥ | å¤šæ ¸æ•°æ®åŠ è½½ã€å¼‚æ­¥æ¨ç†æœåŠ¡ |
| **æ€§èƒ½ä¼˜åŒ–** | å‘é‡åŒ–ã€å†…å­˜ç®¡ç† | åŠ é€Ÿè®­ç»ƒã€å‡å°‘æ˜¾å­˜å ç”¨ |
| **è®¾è®¡æ¨¡å¼** | å·¥å‚ã€ç­–ç•¥ã€å•ä¾‹ | å¤§é¡¹ç›®ä»£ç ç»“æ„ã€æ˜“æ‰©å±•æ€§ |
| **å·¥ç¨‹å®æˆ˜** | Datasetã€è®­ç»ƒæ¡†æ¶ã€éƒ¨ç½² | å®Œæ•´çš„ AI å¼€å‘æµç¨‹ |

æŒæ¡è¿™äº›çŸ¥è¯†ï¼Œä½ å°±èƒ½å†™å‡º**é«˜æ•ˆã€å¯ç»´æŠ¤ã€æ˜“æ‰©å±•**çš„ AI ä»£ç ï¼































