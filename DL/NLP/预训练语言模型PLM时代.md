# 预训练语言模型（PLM）时代（BERT → GPT）

1. 预训练模型的范式
	•	Masked Language Model（MLM：BERT）
		- **是什么**：就像英语考试的“完形填空”。把句子中间挖掉几个词（Mask），让模型根据上下文填出来。
		- **解决什么问题**：强迫模型同时利用“上文”和“下文”的信息，学会了深度的双向语境理解，是 BERT 成功的关键。
	•	AutoRegressive LM（GPT）
		- **是什么**：就像“成语接龙”。根据前面的字，预测下一个字是什么。
		- **解决什么问题**：这是最自然的语言生成方式，让模型学会了流畅地说话和写作。
	•	Next Sentence Prediction（NSP）
		- **是什么**：给模型两个句子，让它判断第二句是不是紧接着第一句说的。
		- **解决什么问题**：帮助模型理解句子与句子之间的逻辑关系，对问答系统（QA）和自然语言推理（NLI）很有用。
	•	Permutation LM（XLNet）
		- **是什么**：乱序语言模型。把句子里的词打乱顺序（逻辑上打乱），然后让模型去预测。
		- **解决什么问题**：试图结合 BERT（双向理解）和 GPT（自回归生成）的优点，解决 BERT 中 [MASK] 标记在微调时不存在的不一致问题。
	•	Prefix LM（T5）
		- **是什么**：前缀语言模型。前一部分（Prefix）是完全可见的（像 BERT），后一部分是逐词生成的（像 GPT）。
		- **解决什么问题**：专门为 Seq2Seq 任务设计，适合翻译、摘要等既需要理解输入又需要生成输出的任务。

2. 经典模型架构
	•	BERT、RoBERTa
		- **是什么**：BERT 是 Google 提出的基于 Transformer Encoder 的双向模型；RoBERTa 是 Facebook 改进的 BERT（去掉了 NSP 任务，用了更多数据，训练更久）。
		- **解决什么问题**：开启了 NLP 的预训练+微调新时代，在几乎所有理解类任务（分类、实体识别）上达到了当时的最优效果。
	•	GPT-2 / GPT-3
		- **是什么**：OpenAI 坚持走的 Decoder-only 路线。GPT-2 证明了模型大到一定程度可以生成连贯长文；GPT-3 证明了不需要微调，只给几个例子（Few-shot）模型就能学会新任务。
		- **解决什么问题**：展示了“大力出奇迹”的扩展定律（Scaling Law），是通向现代 ChatGPT 的基石。
	•	T5
		- **是什么**：“Text-to-Text Transfer Transformer”。它把所有 NLP 任务（翻译、分类、回归）都转换成“输入文本 -> 输出文本”的形式。
		- **解决什么问题**：用统一的框架解决所有 NLP 问题，简化了多任务学习的复杂度。
	•	ALBERT
		- **是什么**：轻量级 BERT。通过参数共享（每一层参数都一样）和矩阵分解技术，大幅减少了参数量。
		- **解决什么问题**：解决了 BERT 模型参数过多、显存占用过大的问题，让模型更轻便。
	•	DistilBERT（模型压缩思想）
		- **是什么**：蒸馏 BERT。用一个大的“老师模型”（BERT）教一个小的“学生模型”（DistilBERT）。
		- **解决什么问题**：在保留 BERT 97% 性能的前提下，把模型缩小了 40%，速度提升了 60%，适合在手机等边缘设备上部署。
	•	ELECTRA（替换采样）
		- **是什么**：不像 BERT 那样“填空”，而是“找茬”。生成器生成一些假词替换原词，ELECTRA 负责把这些假词找出来。
		- **解决什么问题**：比 BERT 训练效率更高，用更少的计算资源就能达到相同的效果。

实践：
	•	HuggingFace Transformers
	•	复现文本分类 / 问答任务
	•	试着做 Prompt-based fine-tuning
