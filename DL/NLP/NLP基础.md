# NLP 基础

**自然语言处理（Natural Language Processing, NLP）** 是计算机科学、人工智能和语言学的交叉领域，致力于让计算机能够理解、处理和生成人类的自然语言。

核心目标：
+ 理解：让计算机能够理解人类语言的含义
+ 处理：对文本和语音进行分析、转换和操作
+ 生成：让计算机能够产生自然、流畅的人类语言

  
## 1 介绍


### 1.1 自然语言的特点
人类语言具有以下独特特征，这些特征使得NLP成为一个极具挑战性的领域：

+ **歧义性（Ambiguity）**
  + **词汇歧义**：一个词有多种含义,例："bank"可以指金融机构，也可以指河岸
  + **句法歧义**：句子的语法结构可以有多种解释,例："我看见了那个拿着望远镜的人"（是人拿着望远镜，还是我用望远镜看见人？）
  + **语义歧义**：句子的整体含义不明确,例："他们买了苹果"（是水果还是苹果公司的产品？）

+ **上下文依赖性（Context Dependency）**
    + 同一个词或句子在不同语境中含义不同,例："这个想法很cool"中的"cool"表示很棒，而"今天很cool"表示凉爽
  
+ **创新性与变化性**
    + 语言不断发展，新词汇、新表达方式层出不穷,网络用语、流行语的快速传播,例：从"给力"到"yyds"（永远的神）

+  **文化和社会背景**
    +  语言承载着深厚的文化内涵,同一语言在不同地区有方言差异,例：中文的"吃了吗？"不仅是询问，更是一种问候方式

+ **非标准化**
   + 口语化表达、缩写、错别字,语法不规范、句子不完整.例：微博、聊天记录中的非正式表达



### 1.2 NLP 的主要任务
+ **基础任务**
  + 分词（Tokenization）：将文本分解为有意义的单元
  + 词性标注（POS Tagging）：标识每个词的语法类别
  + 句法分析（Parsing）：分析句子的语法结构
  + 命名实体识别（NER）：识别人名、地名、机构名等

+ **理解任务**
  + 语义角色标注：识别句子中的语义关系
  + 共指消解：确定文本中指向同一实体的不同表达
  + 关系抽取：识别实体间的语义关系
  + 事件抽取：从文本中抽取事件信息

+ **应用任务**
  + 文本分类：将文本归类到预定义类别
  + 情感分析：判断文本的情感倾向
  + 机器翻译：将一种语言翻译成另一种语言
  + 文本摘要：生成文本的简洁总结
  + 问答系统：根据问题检索或生成答案

### 发展历史...


## 2 文本预处理
文本预处理是自然语言处理（NLP）中的基础且关键步骤，它将原始的非结构化文本数据转化为适合机器学习模型处理的格式。

本文将系统介绍文本预处理的三大核心环节：**文本清洗、分词和词性标注。**

### 2.1 文本清洗：净化原始文本数据
文本清洗是预处理的第一步，目的是去除文本中的噪声数据，提高后续处理的准确性。

#### 2.1.1 编码格式处理
不同来源的文本可能采用不同的编码格式（如UTF-8、GBK、ASCII等），统一编码是首要任务：

```python
# 编码转换示例
text = "示例文本".encode('gbk')  # 假设原始编码是GBK
text = text.decode('gbk').encode('utf-8')  # 转换为UTF-8
```
常见编码问题解决方案：
+ 使用chardet库自动检测编码
+ 统一转换为UTF-8编码
+ 处理无法解码的字符（通常替换或忽略）


#### 2.1.2 特殊字符处理
不同场景下需要处理不同类型的特殊字符：

| 字符类型 | 处理方法               | 应用场景       |
|----------|------------------------|----------------|
| HTML标签 | 正则表达式移除         | 网页爬取文本   |
| 表情符号 | 移除或转换为文字描述   | 社交媒体分析   |
| 控制字符 | 过滤掉                 | 所有文本处理   |
| 特殊标点 | 标准化处理             | 文本规范化     |

```python
import re

# 移除HTML标签示例
text = "<p>这是一段<b>HTML</b>文本</p>"
clean_text = re.sub(r'<[^>]+>', '', text)
print(clean_text)  # 输出: 这是一段HTML文本
```


#### 2.1.3 噪声数据去除
根据具体任务需求，可能需要：
+ 去除无关信息（广告、版权声明等）
+ 处理拼写错误（使用拼写检查库）
+ 标准化数字表示（如将"1000"统一为"1,000"）
+ 统一日期格式（"2023-01-01" vs "01/01/2023"）


### 2.2 分词（Tokenization）
分词是将连续文本分割成有意义的语言单元（token）的过程，不同语言需要不同的分词方法。

#### 2.2.1 英文分词方法
英文分词相对简单，主要基于空格和标点分割：
```python
# 使用NLTK进行英文分词
from nltk.tokenize import word_tokenize

text = "Natural Language Processing is fascinating!"
tokens = word_tokenize(text)
print(tokens)  # ['Natural', 'Language', 'Processing', 'is', 'fascinating', '!']
```

英文分词注意事项：
+ 处理缩写（如"I'm"→"I"+"'m"）
+ 保留或合并特定短语（如"New York"作为一个token）
+ 处理连字符（"state-of-the-art"）

#### 2.2.2 中文分词技术
中文没有明显的词边界，分词更为复杂。主要方法包括：

+ 基于词典的分词：最大匹配法、最短路径法
+ 基于统计的分词：HMM、CRF等序列标注方法
+ 基于深度学习的分词：BiLSTM-CRF、BERT等模型

```python
# 使用jieba进行中文分词
import jieba

text = "自然语言处理非常有趣"
tokens = jieba.lcut(text)
print(tokens)  # ['自然语言', '处理', '非常', '有趣']
```

## NLP 基础知识
	•	Token / Type / Vocabulary
		- **是什么**：Token 是文本处理的最小单位（如单词、字符或子词）；Type 是不重复的 Token 类型；Vocabulary 是所有 Type 的集合（词表）。
		- **解决什么问题**：计算机无法直接理解连续的文本流，需要将其“切分”成离散的、可计算的单元，建立了人类语言与机器索引之间的映射。
	•	语言模型（LM）的概念
		- **是什么**：一种计算文本序列概率的模型，简单说就是根据上文预测下一个词出现的概率。
		- **解决什么问题**：判断一句话是否通顺（如拼写纠错、语音识别纠错），或者生成通顺的文本（如机器翻译、文本生成）。
	•	词法分析、句法分析
		- **是什么**：词法分析包括分词、词性标注（判断是名词还是动词）；句法分析是分析句子中词与词之间的依赖关系（如主谓宾结构）。
		- **解决什么问题**：让计算机理解语言的语法结构，消除歧义。例如，“我喜欢一个人”是喜欢某个人，还是喜欢独处？句法分析有助于区分。
	•	N-Gram、统计语言模型
		- **是什么**：基于统计的语言模型，假设当前词只与前面 N-1 个词有关。
		- **解决什么问题**：在深度学习出现前，这是最主流的建模文本概率的方法，用于解决数据稀疏问题，计算简单但难以捕捉长距离依赖。
	•	常见 NLP 任务分类（分类、生成、结构化预测…）
		- **是什么**：
			- 分类：情感分析（好评/差评）、垃圾邮件识别。
			- 生成：机器翻译、对话系统、文章摘要。
			- 结构化预测：命名实体识别（NER，从文本中提取人名、地名）。
		- **解决什么问题**：明确不同的业务需求对应什么类型的算法模型。

推荐学习方式：
	•	《Speech and Language Processing（3rd）》前几章
	•	Stanford NLP 基础课程 CS224N（前两讲）

⸻

2. 文本特征表示（Embedding 演化线）

从最传统到现代嵌入表征，理解演进脉络非常重要：
	•	Bag-of-Words （BoW）
		- **是什么**：词袋模型，将文本看作一袋子词，只统计词出现的次数，忽略顺序。
		- **解决什么问题**：最早期的文本向量化方法，解决了如何把文本变成计算机能算的数字向量的问题。但缺点是丢失了语序和语义信息。
	•	TF-IDF
		- **是什么**：在词频（TF）的基础上，引入逆文档频率（IDF），给常见的词（如“的”、“是”）降权，给稀有的、能代表文章主题的词加权。
		- **解决什么问题**：解决了 BoW 中“常用词频率高但无实际意义”的问题，能更好地提取文章的关键词。
	•	Word2Vec（CBOW、SkipGram）
		- **是什么**：一种静态词向量技术，通过神经网络训练，将每个词映射到一个稠密的向量空间，使得语义相似的词在空间中距离更近（如“国王”和“王后”距离近）。
		- **解决什么问题**：解决了“词汇鸿沟”问题，让计算机能理解词与词之间的语义相似度，且维度更低，计算更高效。
	•	GloVe
		- **是什么**：结合了全局矩阵分解（统计共现信息）和局部上下文窗口优势的词向量方法。
		- **解决什么问题**：试图同时利用全局统计信息（TF-IDF的强项）和局部上下文信息（Word2Vec的强项）来生成更好的词向量。
	•	FastText
		- **是什么**：Facebook 提出的基于子词（Subword）的词向量模型，将单词拆解成 n-gram 字符片段（如 apple -> ap, pp, pl, le）。
		- **解决什么问题**：解决了 Word2Vec 无法处理未登录词（OOV）的问题，对于形态变化丰富的语言（如英语、德语）效果很好。
	•	Contextual Embedding（ELMo、ULMFiT → Transformer）
		- **是什么**：动态词向量，同一个词在不同语境下有不同的向量表示（如“苹果”在“吃苹果”和“苹果手机”中表示不同）。
		- **解决什么问题**：解决了静态词向量（Word2Vec）无法解决的“一词多义”问题，是通向现代预训练模型（BERT/GPT）的关键一步。

建议实践：
	•	用 sklearn + gensim 实现 TF-IDF、Word2Vec
	•	小规模文本分类任务（如 IMDB）
